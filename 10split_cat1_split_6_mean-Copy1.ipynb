{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into 20 categories (10-90% percentile for positive and negative so 10x2) based on SMA 1 minute from past 20 days, \n",
    "#then within each SMA category, separate into quartiles based on cumulative volume in 1 minute\n",
    "#calculate the mean and std of the edge in each case\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "os.chdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list=os.listdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '2019.01.02.csv',\n",
       " '2019.01.03.csv',\n",
       " '2019.01.04.csv',\n",
       " '2019.01.07.csv',\n",
       " '2019.01.08.csv',\n",
       " '2019.01.09.csv',\n",
       " '2019.01.10.csv',\n",
       " '2019.01.11.csv',\n",
       " '2019.01.14.csv',\n",
       " '2019.01.15.csv',\n",
       " '2019.01.16.csv',\n",
       " '2019.01.17.csv',\n",
       " '2019.01.18.csv',\n",
       " '2019.01.21.csv',\n",
       " '2019.01.22.csv',\n",
       " '2019.01.23.csv',\n",
       " '2019.01.24.csv',\n",
       " '2019.01.25.csv',\n",
       " '2019.01.28.csv',\n",
       " '2019.01.29.csv',\n",
       " '2019.01.30.csv',\n",
       " '2019.01.31.csv',\n",
       " '2019.02.01.csv',\n",
       " '2019.02.11.csv',\n",
       " '2019.02.12.csv',\n",
       " '2019.02.13.csv',\n",
       " '2019.02.14.csv',\n",
       " '2019.02.15.csv',\n",
       " '2019.02.18.csv',\n",
       " '2019.02.19.csv',\n",
       " '2019.02.20.csv',\n",
       " '2019.02.21.csv',\n",
       " '2019.02.22.csv',\n",
       " '2019.02.25.csv',\n",
       " '2019.02.26.csv',\n",
       " '2019.02.27.csv',\n",
       " '2019.02.28.csv',\n",
       " '2019.03.01.csv',\n",
       " '2019.03.04.csv',\n",
       " '2019.03.05.csv',\n",
       " '2019.03.06.csv',\n",
       " '2019.03.07.csv',\n",
       " '2019.03.08.csv',\n",
       " '2019.03.11.csv',\n",
       " '2019.03.12.csv',\n",
       " '2019.03.13.csv',\n",
       " '2019.03.14.csv',\n",
       " '2019.03.15.csv',\n",
       " '2019.03.18.csv',\n",
       " '2019.03.19.csv',\n",
       " '2019.03.20.csv',\n",
       " '2019.03.21.csv',\n",
       " '2019.03.22.csv',\n",
       " '2019.03.25.csv',\n",
       " '2019.03.26.csv',\n",
       " '2019.03.27.csv',\n",
       " '2019.03.28.csv',\n",
       " '2019.03.29.csv',\n",
       " '2019.04.01.csv',\n",
       " '2019.04.02.csv',\n",
       " '2019.04.03.csv',\n",
       " '2019.04.04.csv',\n",
       " '2019.04.08.csv',\n",
       " '2019.04.09.csv',\n",
       " '2019.04.10.csv',\n",
       " '2019.04.11.csv',\n",
       " '2019.04.12.csv',\n",
       " '2019.04.15.csv',\n",
       " '2019.04.16.csv',\n",
       " '2019.04.17.csv',\n",
       " '2019.04.18.csv',\n",
       " '2019.04.19.csv',\n",
       " '2019.04.22.csv',\n",
       " '2019.04.23.csv',\n",
       " '2019.04.24.csv',\n",
       " '2019.04.25.csv',\n",
       " '2019.04.26.csv',\n",
       " '2019.04.29.csv',\n",
       " '2019.04.30.csv',\n",
       " '2019.05.06.csv',\n",
       " '2019.05.07.csv',\n",
       " '2019.05.08.csv',\n",
       " '2019.05.09.csv',\n",
       " 'results']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorise():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(7)\n",
    "                else:\n",
    "                    temp.append(8)\n",
    "            if i<0:\n",
    "                if i>self.threshold[0][2]:\n",
    "                    temp.append(4)\n",
    "                elif i>self.threshold[0][1]:\n",
    "                    temp.append(3)\n",
    "                elif i>self.threshold[0][0]:\n",
    "                    temp.append(2)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "        return np.asarray(temp)\n",
    "    \n",
    "class categorise_simple():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        self.threshold.append(np.percentile(array,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                else:\n",
    "                    temp.append(4)\n",
    "        return np.asarray(temp)    \n",
    "    \n",
    "class categorise_10():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[10,20,30,40,50,60,70,80,90]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(11)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(12)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(13)\n",
    "                elif i<self.threshold[1][3]:\n",
    "                    temp.append(14)\n",
    "                elif i<self.threshold[1][4]:\n",
    "                    temp.append(15)\n",
    "                elif i<self.threshold[1][5]:\n",
    "                    temp.append(16)\n",
    "                elif i<self.threshold[1][6]:\n",
    "                    temp.append(17)\n",
    "                elif i<self.threshold[1][7]:\n",
    "                    temp.append(18)\n",
    "                elif i<self.threshold[1][8]:\n",
    "                    temp.append(19)                    \n",
    "                else:\n",
    "                    temp.append(20)\n",
    "            if i<0:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                elif i<self.threshold[0][3]:\n",
    "                    temp.append(4)\n",
    "                elif i<self.threshold[0][4]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[0][5]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[0][6]:\n",
    "                    temp.append(7)\n",
    "                elif i<self.threshold[0][7]:\n",
    "                    temp.append(8)\n",
    "                elif i<self.threshold[0][8]:\n",
    "                    temp.append(9)                    \n",
    "                else:\n",
    "                    temp.append(10)\n",
    "        return np.asarray(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smart_price(dataset):\n",
    "    data=dataset[:]\n",
    "    \n",
    "    #to combat the limit up event, where price is set to 0. \n",
    "    rows=(data.loc[:,'BidPrice1']==0) #count rows of bid price equal 0\n",
    "    if (np.any(rows)): #if there is such a row\n",
    "        data.at[rows,'BidPrice1']=data.loc[rows,'AskPrice1'] #for that row, assign ask price to it\n",
    "    rows=(data.loc[:,'AskPrice1']==0) #do the same for ask price\n",
    "    if (np.any(rows)):\n",
    "        data.at[rows,'AskPrice1']=data.loc[rows,'BidPrice1'] \n",
    "        \n",
    "    data['smart_price']=data.loc[:,'BidPrice1']*data.loc[:,'AskVol1']+data.loc[:,'AskPrice1']*data.loc[:,'BidVol1']\n",
    "    data.at[:,'smart_price']=data.loc[:,'smart_price']/(data.loc[:,['BidVol1','AskVol1']].sum(axis=1))  \n",
    "    return data\n",
    "\n",
    "def calc_present_vol(dataset):\n",
    "    data=dataset[:]\n",
    "    data['current_vol']=data.loc[:,'Volume'].diff().fillna(0)/2\n",
    "    return data\n",
    "\n",
    "def calc_future_price(dataset,time_ahead=30):\n",
    "    data=dataset[:]\n",
    "    future_price=[]\n",
    "    length=len(data)\n",
    "    for i in range(len(data)):\n",
    "        current_time=data[i,44]+timedelta(seconds=time_ahead)\n",
    "        #print(data[i,44])\n",
    "        j=0\n",
    "        #print(current_time)\n",
    "        while((i+j)<length and current_time>data[(i+j),44]):\n",
    "            j+=1\n",
    "        #print(i,j,(data[(i+j-1),44]))\n",
    "        if (i+j)<length:\n",
    "            future_price.append(data[(i+j),51]) #51 is the index for smart price            \n",
    "        else:\n",
    "            future_price.append(np.nan)\n",
    "    future_price=np.asarray(future_price)\n",
    "    future_price=np.expand_dims(future_price,axis=1)\n",
    "    return np.concatenate((data,future_price),axis=1)\n",
    "\n",
    "\n",
    "def calc_edge(dataset):\n",
    "    data=dataset.copy()\n",
    "    temp=data[:,53]-data[:,51]\n",
    "    temp=np.expand_dims(temp,axis=1)\n",
    "    return np.concatenate((data,temp),axis=1)\n",
    "\n",
    "def set_index(dataset):\n",
    "    data=dataset[:]\n",
    "    index=data[:,44]\n",
    "    new_index=[]\n",
    "    for j in range(len(index)):\n",
    "        i=str(index[j]*1000)\n",
    "        if len(i)==11:\n",
    "            i='0'+i\n",
    "        i=i[:-10]+':'+i[-10:]\n",
    "        i=i[:-8]+':'+i[-8:]\n",
    "        i=i[:-6]+':'+i[-6:]\n",
    "        new_index.append(datetime.strptime(i,\"%H:%M:%S:%f\"))\n",
    "    data[:,44]=new_index\n",
    "    return data\n",
    "\n",
    "def calc_sma_fast(dataset,duration=1): #faster way to calculate SMA, 0.05 seconds for 5000 rows\n",
    "    data=dataset[:]\n",
    "    sma_values=[] \n",
    "    smart_sum=np.cumsum(data[:,51])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            sma=(smart_sum[i]-smart_sum[i-j])/(j)\n",
    "            sma_values.append(sma)\n",
    "        else:\n",
    "            sma=smart_sum[i]/(i+1)\n",
    "            sma_values.append(sma)\n",
    "\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=data[:,51]-sma_values\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1)  \n",
    "\n",
    "def calc_past_vol(dataset,duration=1): #\n",
    "    data=dataset[:]\n",
    "    vol_values=[] \n",
    "    vol_sum=np.cumsum(data[:,52])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            vol=(vol_sum[i]-vol_sum[i-j])\n",
    "            vol_values.append(vol)\n",
    "        else:\n",
    "            vol=vol_sum[i]\n",
    "            vol_values.append(vol)\n",
    "    vol_values=np.asarray(vol_values)\n",
    "    vol_values=np.expand_dims(vol_values,axis=1)\n",
    "    return np.concatenate((data,vol_values),axis=1) #52  \n",
    "\n",
    "def process(dataset,sma_duration=1,vol_duration=1):\n",
    "    data=dataset[:]\n",
    "    data=calc_smart_price(data) #51\n",
    "    data=calc_present_vol(data).values #52\n",
    "    data=set_index(data)\n",
    "    data=calc_future_price(data) #53\n",
    "    data=calc_edge(data) #54\n",
    "    data=calc_sma_fast(data,duration=sma_duration) #55\n",
    "    data=calc_past_vol(data,duration=vol_duration) #56\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.02.csv read\n",
      "2019.01.03.csv read\n",
      "2019.01.04.csv read\n",
      "2019.01.07.csv read\n",
      "2019.01.08.csv read\n",
      "2019.01.09.csv read\n",
      "2019.01.10.csv read\n",
      "2019.01.11.csv read\n",
      "2019.01.14.csv read\n",
      "2019.01.15.csv read\n",
      "2019.01.16.csv read\n",
      "2019.01.17.csv read\n",
      "2019.01.18.csv read\n",
      "2019.01.21.csv read\n",
      "2019.01.22.csv read\n",
      "2019.01.23.csv read\n",
      "2019.01.24.csv read\n",
      "2019.01.25.csv read\n",
      "2019.01.28.csv read\n",
      "2019.01.29.csv read\n",
      "2019.01.30.csv read\n",
      "2019.01.31.csv read\n",
      "2019.02.01.csv read\n",
      "2019.02.11.csv read\n",
      "2019.02.12.csv read\n",
      "2019.02.13.csv read\n",
      "2019.02.14.csv read\n",
      "2019.02.15.csv read\n",
      "2019.02.18.csv read\n",
      "2019.02.19.csv read\n",
      "2019.02.20.csv read\n",
      "2019.02.21.csv read\n",
      "2019.02.22.csv read\n",
      "2019.02.25.csv read\n",
      "2019.02.26.csv read\n",
      "2019.02.27.csv read\n",
      "2019.02.28.csv read\n",
      "2019.03.01.csv read\n",
      "2019.03.04.csv read\n",
      "2019.03.05.csv read\n",
      "2019.03.06.csv read\n",
      "2019.03.07.csv read\n",
      "2019.03.08.csv read\n",
      "2019.03.11.csv read\n",
      "2019.03.12.csv read\n",
      "2019.03.13.csv read\n",
      "2019.03.14.csv read\n",
      "2019.03.15.csv read\n",
      "2019.03.18.csv read\n",
      "2019.03.19.csv read\n",
      "2019.03.20.csv read\n",
      "2019.03.21.csv read\n",
      "2019.03.22.csv read\n",
      "2019.03.25.csv read\n",
      "2019.03.26.csv read\n",
      "2019.03.27.csv read\n",
      "2019.03.28.csv read\n",
      "2019.03.29.csv read\n",
      "2019.04.01.csv read\n",
      "2019.04.02.csv read\n",
      "2019.04.03.csv read\n",
      "2019.04.04.csv read\n",
      "2019.04.08.csv read\n",
      "2019.04.09.csv read\n",
      "2019.04.10.csv read\n",
      "2019.04.11.csv read\n",
      "2019.04.12.csv read\n",
      "2019.04.15.csv read\n",
      "2019.04.16.csv read\n",
      "2019.04.17.csv read\n",
      "2019.04.18.csv read\n",
      "2019.04.19.csv read\n",
      "2019.04.22.csv read\n",
      "2019.04.23.csv read\n",
      "2019.04.24.csv read\n",
      "2019.04.25.csv read\n",
      "2019.04.26.csv read\n",
      "2019.04.29.csv read\n",
      "2019.04.30.csv read\n",
      "2019.05.06.csv read\n",
      "2019.05.07.csv read\n",
      "2019.05.08.csv read\n",
      "2019.05.09.csv read\n"
     ]
    }
   ],
   "source": [
    "df_list=[]\n",
    "name_list=[]\n",
    "path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/'\n",
    "for file in file_list: #read all files and add them to file_list\n",
    "    if file[-3:]=='csv': #check if file is a CSV\n",
    "        name_list.append(file)\n",
    "        df_list.append(process(pd.read_csv(path+file),sma_duration=10))\n",
    "        print(file,'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/'\n",
    "\n",
    "final_df=pd.DataFrame()    \n",
    "\n",
    "for i in range(len(df_list)): #for each 20 day rolling window\n",
    "    if i<19:\n",
    "        continue\n",
    "    print(name_list[i])\n",
    "\n",
    "    #get -19 day\n",
    "    sma=df_list[i-19][:,55].copy() #column for SMA\n",
    "    vol=df_list[i-19][:,56].copy()\n",
    "    \n",
    "    #get -18 to 0 day (19 days in total)\n",
    "    for k in range((i-18),i+1): #get 20 day moving averages\n",
    "        sma=np.concatenate((sma,df_list[k][:,55].copy()))\n",
    "        vol=np.concatenate((vol,df_list[k][:,56].copy()))\n",
    "        \n",
    "    cat_sma=categorise_10()\n",
    "    cat_sma.fit(sma) #calculate quartile thresholds for past 20 days\n",
    "    \n",
    "    #get categories for past 20 days\n",
    "    cat_sma_20=(cat_sma.return_quartile(sma))   \n",
    "\n",
    "    #get x,y for regression\n",
    "    x_today=df_list[i][:,55].copy().astype(float) #column for SMA     \n",
    "    y_today=df_list[i][:,54].copy().astype(float) #column for edge\n",
    "    \n",
    "    vol_today=df_list[i][:,56].copy()    \n",
    "    #removing all NA\n",
    "    isnum=(~np.isnan(x_today)) & (~np.isnan(y_today))\n",
    "    #get categories of today's sma  \n",
    "    cat_x_today=cat_sma.return_quartile(x_today)\n",
    "    \n",
    "    \n",
    "    reg_result={}\n",
    "    reg_result['date']=name_list[i]\n",
    "    reg_result['total_obs']=len(x_today[isnum])   \n",
    "    \n",
    "    for cat in range(1,21):\n",
    "        #check past 20 days quartiles and filter volume by category\n",
    "        vol_fit=vol[cat_sma_20==cat].copy()\n",
    "\n",
    "        #simple quartile categorisation\n",
    "        cat_vol=categorise_simple()\n",
    "        cat_vol.fit(vol_fit)\n",
    "    \n",
    "        \n",
    "        #today's sma filter\n",
    "        sma_filter_today=(cat_x_today==cat)\n",
    "        #today's vol categories\n",
    "        cat_vol_today=cat_vol.return_quartile(vol_today)\n",
    "\n",
    "\n",
    "        for quartile in [1,2,3,4]:\n",
    "\n",
    "            filtered= (isnum & sma_filter_today) #filtering NA and sma quartile\n",
    "            filtered= filtered & (cat_vol_today==quartile) #filtering volume\n",
    "            new_y=y_today[filtered].copy()\n",
    "\n",
    "            q='ma_cat_'+str(cat)+'_vol_quartile'+str(quartile)+'_'\n",
    "            if (len(new_y)!=0):\n",
    "\n",
    "                reg_result[(q+'mean')]=np.mean(new_y)\n",
    "                reg_result[(q+'std')]=np.std(new_y)\n",
    "                reg_result[(q+'num_obs')]=len(new_y)\n",
    "\n",
    "            else:\n",
    "\n",
    "                reg_result[(q+'mean')]=np.nan\n",
    "                reg_result[(q+'std')]=np.nan\n",
    "                reg_result[(q+'num_obs')]='0'\n",
    "\n",
    "    final_df=final_df.append(reg_result,ignore_index=True)                \n",
    "temp=df_path+'result_10sma,6vol_1min,10min_sma,vol_mean.csv'\n",
    "final_df.to_csv(temp)\n",
    "print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      " [[ 0.07843797 -0.23048597  0.11534966  0.5988208 ]\n",
      " [-0.13283748  0.05105564 -0.17884915  0.02694325]\n",
      " [-0.1587211  -0.06577319 -0.24579428 -0.05004354]\n",
      " [-0.20478467 -0.06841722 -0.10002672  0.01562091]\n",
      " [-0.24717687 -0.17034711 -0.22237342  0.07284985]\n",
      " [-0.23010979 -0.18772596 -0.12555671 -0.05189646]\n",
      " [-0.15663821 -0.16381254 -0.05347152 -0.04049586]\n",
      " [-0.25274037 -0.21139116 -0.10949279 -0.01679178]\n",
      " [-0.21805336 -0.17166978 -0.151711   -0.01616515]\n",
      " [-0.07549431 -0.05488275  0.0194412  -0.02360775]\n",
      " [-0.0090904  -0.06663312  0.01868619  0.08437054]\n",
      " [ 0.12454442 -0.00096759  0.00281398  0.11445082]\n",
      " [ 0.0739863   0.17285096  0.02678769  0.07144233]\n",
      " [ 0.06699675  0.17697387  0.1097174  -0.02173619]\n",
      " [ 0.08352516  0.15526393  0.00912661  0.10020403]\n",
      " [ 0.27470055  0.13745263  0.08384805  0.1344533 ]\n",
      " [ 0.16335977  0.19592326  0.13778884  0.07450819]\n",
      " [ 0.25912746  0.19047582  0.09879137 -0.00971468]\n",
      " [ 0.39241841  0.16436613 -0.0012471   0.11328626]\n",
      " [ 0.27434412  0.20106498  0.11130333 -0.55100751]]\n",
      "std\n",
      " [[0.81666293 1.07147264 1.42795699 1.99798399]\n",
      " [0.72062379 0.63436796 0.87441863 1.15935657]\n",
      " [0.58187451 0.69075876 0.64994111 0.90532909]\n",
      " [0.52192267 0.53766252 0.53654056 0.72969121]\n",
      " [0.58380949 0.40843532 0.54491685 0.64262911]\n",
      " [0.59980968 0.56760353 0.51912693 0.63105813]\n",
      " [0.41161563 0.50266522 0.45824383 0.55179226]\n",
      " [0.43911721 0.39405535 0.46537358 0.53216224]\n",
      " [0.49476592 0.45016433 0.55343711 0.52152416]\n",
      " [0.71629531 0.48085381 0.41653915 0.58461097]\n",
      " [0.54592702 0.41313902 0.5529749  0.43695415]\n",
      " [0.6772586  0.51366581 0.4577912  0.4788461 ]\n",
      " [0.42865417 0.43551561 0.53286702 0.47069441]\n",
      " [0.48342021 0.47696763 0.49906535 0.57475759]\n",
      " [0.42366334 0.46401542 0.49408411 0.61613719]\n",
      " [0.47974872 0.55317661 0.53686165 0.61508104]\n",
      " [0.60512955 0.54984459 0.51317119 0.68124209]\n",
      " [0.67196172 0.62569575 0.70920833 0.89269756]\n",
      " [0.79897134 0.88384905 0.8892162  0.95542909]\n",
      " [1.00214039 1.24569381 1.6620614  2.04908732]]\n",
      "% positive\n",
      " [[0.546875 0.5      0.5625   0.515625]\n",
      " [0.4375   0.53125  0.515625 0.609375]\n",
      " [0.390625 0.53125  0.34375  0.5     ]\n",
      " [0.296875 0.375    0.390625 0.453125]\n",
      " [0.3125   0.328125 0.359375 0.53125 ]\n",
      " [0.28125  0.34375  0.375    0.484375]\n",
      " [0.328125 0.375    0.375    0.4375  ]\n",
      " [0.21875  0.28125  0.4375   0.484375]\n",
      " [0.28125  0.328125 0.34375  0.5     ]\n",
      " [0.46875  0.46875  0.484375 0.53125 ]\n",
      " [0.515625 0.40625  0.4375   0.578125]\n",
      " [0.46875  0.46875  0.578125 0.609375]\n",
      " [0.5625   0.625    0.546875 0.5625  ]\n",
      " [0.546875 0.65625  0.59375  0.5625  ]\n",
      " [0.578125 0.625    0.53125  0.625   ]\n",
      " [0.703125 0.5625   0.546875 0.578125]\n",
      " [0.59375  0.671875 0.578125 0.5625  ]\n",
      " [0.578125 0.609375 0.53125  0.484375]\n",
      " [0.640625 0.5625   0.5625   0.59375 ]\n",
      " [0.546875 0.578125 0.5      0.359375]]\n",
      "num obs\n",
      " [[35187. 33627. 34703. 35477.]\n",
      " [39495. 34581. 34292. 35548.]\n",
      " [39821. 37442. 35906. 35382.]\n",
      " [43142. 38399. 35974. 36180.]\n",
      " [44236. 38759. 36089. 36015.]\n",
      " [45297. 40590. 37614. 36428.]\n",
      " [44753. 41298. 37896. 36934.]\n",
      " [45921. 43096. 38527. 37340.]\n",
      " [49032. 42870. 38887. 36079.]\n",
      " [52512. 43428. 39272. 37449.]\n",
      " [47805. 45372. 41060. 38248.]\n",
      " [49132. 41262. 37852. 36328.]\n",
      " [46498. 41015. 37887. 36149.]\n",
      " [45210. 39216. 37078. 36243.]\n",
      " [45187. 39091. 37146. 35893.]\n",
      " [43931. 38597. 35150. 35799.]\n",
      " [40911. 36652. 35251. 35022.]\n",
      " [39204. 35277. 35305. 35527.]\n",
      " [37017. 33831. 34633. 34439.]\n",
      " [33772. 33921. 31703. 36223.]]\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/10 sma 6 volume/result_10sma,6vol_1min,1min_sma,vol_mean.csv')\n",
    "x=[]\n",
    "x_std=[]\n",
    "pos=[]\n",
    "num_obs=[]\n",
    "for ma_cat in range(1,21):\n",
    "    for vol_cat in range(1,5):\n",
    "        col='ma_cat_'+str(ma_cat)+'_vol_quartile'+str(vol_cat)+'_mean'\n",
    "        std=data.loc[:,col].std()\n",
    "        mean=data.loc[:,col].mean()\n",
    "        column=data.loc[:,col]\n",
    "        x_std.append(std)\n",
    "        x.append(mean)\n",
    "        percentage=len(column[column>=0])/len(column)\n",
    "        pos.append(percentage)  \n",
    "        \n",
    "        obs='ma_cat_'+str(ma_cat)+'_vol_quartile'+str(vol_cat)+'_num_obs'\n",
    "        obs=data.loc[:,obs].sum()\n",
    "        num_obs.append(obs)\n",
    "x=np.asarray(x)\n",
    "x=np.reshape(x,(20,4))\n",
    "x_std=np.asarray(x_std)\n",
    "x_std=np.reshape(x_std,(20,4))\n",
    "pos=np.asarray(pos)\n",
    "pos=np.reshape(pos,(20,4))\n",
    "num_obs=np.asarray(num_obs)\n",
    "num_obs=np.reshape(num_obs,(20,4))\n",
    "print('mean\\n',x)\n",
    "print('std\\n',x_std)\n",
    "print('% positive\\n',pos)\n",
    "print('num obs\\n',num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
