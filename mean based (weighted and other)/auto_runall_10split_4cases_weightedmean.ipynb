{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "dir_path='/Users/hudsonyeo/Desktop/Python/leo/data/day'\n",
    "os.chdir('/Users/hudsonyeo/Desktop/Python/leo/data/day')\n",
    "import time\n",
    "def process_dir(dir_list):\n",
    "    x=[]\n",
    "    for dir_ in dir_list:\n",
    "        if dir_!='.DS_Store':\n",
    "            x.append(dir_path+'/'+dir_+'/')\n",
    "    return x\n",
    "standard_columns=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/2019.01.02.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'RM', 'SR', 'TA', 'c', 'fu', 'm', 'ni', 'zn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/hudsonyeo/Desktop/Python/leo/data/day/RM/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/SR/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/c/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/fu/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/m/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/ni/',\n",
       " '/Users/hudsonyeo/Desktop/Python/leo/data/day/zn/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list_names=os.listdir(dir_path)\n",
    "dir_list_names.sort()\n",
    "print(dir_list_names)\n",
    "dir_list=process_dir(dir_list_names)\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019.04.23.csv',\n",
       " '2019.01.07.csv',\n",
       " '2019.04.22.csv',\n",
       " '2019.03.15.csv',\n",
       " '2019.04.08.csv',\n",
       " '2019.03.01.csv',\n",
       " '2019.03.29.csv',\n",
       " '2019.01.10.csv',\n",
       " '2019.01.04.csv',\n",
       " '2019.01.11.csv',\n",
       " '2019.03.28.csv',\n",
       " '2019.04.09.csv',\n",
       " '2019.03.14.csv',\n",
       " '2019.04.25.csv',\n",
       " '2019.04.19.csv',\n",
       " '2019.03.04.csv',\n",
       " '2019.01.29.csv',\n",
       " '2019.01.15.csv',\n",
       " '.DS_Store',\n",
       " '2019.01.14.csv',\n",
       " '2019.01.28.csv',\n",
       " '2019.03.05.csv',\n",
       " '2019.04.18.csv',\n",
       " '2019.03.11.csv',\n",
       " '2019.04.30.csv',\n",
       " '2019.04.24.csv',\n",
       " '2019.04.26.csv',\n",
       " '2019.03.07.csv',\n",
       " '2019.03.13.csv',\n",
       " '2019.01.02.csv',\n",
       " '2019.01.16.csv',\n",
       " '2019.01.17.csv',\n",
       " '2019.01.03.csv',\n",
       " '2019.03.12.csv',\n",
       " '2019.03.06.csv',\n",
       " '2019.02.21.csv',\n",
       " '2019.02.20.csv',\n",
       " '2019.02.22.csv',\n",
       " '2019.02.27.csv',\n",
       " '2019.05.06.csv',\n",
       " '2019.05.07.csv',\n",
       " '2019.02.26.csv',\n",
       " '2019.02.18.csv',\n",
       " '2019.02.19.csv',\n",
       " '2019.02.25.csv',\n",
       " '2019.02.28.csv',\n",
       " '2019.02.14.csv',\n",
       " '2019.05.09.csv',\n",
       " '2019.05.08.csv',\n",
       " '2019.02.01.csv',\n",
       " '2019.02.15.csv',\n",
       " 'results',\n",
       " '2019.02.12.csv',\n",
       " '2019.02.13.csv',\n",
       " '2019.02.11.csv',\n",
       " '2019.04.16.csv',\n",
       " '2019.04.02.csv',\n",
       " '2019.04.03.csv',\n",
       " '2019.04.17.csv',\n",
       " '2019.03.22.csv',\n",
       " '2019.04.29.csv',\n",
       " '2019.03.20.csv',\n",
       " '2019.03.08.csv',\n",
       " '2019.04.01.csv',\n",
       " '2019.04.15.csv',\n",
       " '2019.01.31.csv',\n",
       " '2019.01.25.csv',\n",
       " '2019.01.18.csv',\n",
       " '2019.01.24.csv',\n",
       " '2019.01.30.csv',\n",
       " '2019.03.21.csv',\n",
       " '2019.04.04.csv',\n",
       " '2019.03.19.csv',\n",
       " '2019.04.10.csv',\n",
       " '2019.03.25.csv',\n",
       " '2019.01.08.csv',\n",
       " '2019.01.21.csv',\n",
       " '2019.01.09.csv',\n",
       " '2019.03.18.csv',\n",
       " '2019.04.11.csv',\n",
       " '2019.03.26.csv',\n",
       " '2019.01.23.csv',\n",
       " '2019.01.22.csv',\n",
       " '2019.03.27.csv',\n",
       " '2019.04.12.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_list('/Users/hudsonyeo/Desktop/Python/leo/data/day/RM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(dir_path):\n",
    "    return os.listdir(dir_path)\n",
    "\n",
    "class categorise():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(7)\n",
    "                else:\n",
    "                    temp.append(8)\n",
    "            if i<0:\n",
    "                if i>self.threshold[0][2]:\n",
    "                    temp.append(4)\n",
    "                elif i>self.threshold[0][1]:\n",
    "                    temp.append(3)\n",
    "                elif i>self.threshold[0][0]:\n",
    "                    temp.append(2)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "        return np.asarray(temp)\n",
    "    \n",
    "class categorise_simple():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        self.threshold.append(np.percentile(array,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                else:\n",
    "                    temp.append(4)\n",
    "        return np.asarray(temp)    \n",
    "    \n",
    "class categorise_10():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[10,20,30,40,50,60,70,80,90]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(11)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(12)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(13)\n",
    "                elif i<self.threshold[1][3]:\n",
    "                    temp.append(14)\n",
    "                elif i<self.threshold[1][4]:\n",
    "                    temp.append(15)\n",
    "                elif i<self.threshold[1][5]:\n",
    "                    temp.append(16)\n",
    "                elif i<self.threshold[1][6]:\n",
    "                    temp.append(17)\n",
    "                elif i<self.threshold[1][7]:\n",
    "                    temp.append(18)\n",
    "                elif i<self.threshold[1][8]:\n",
    "                    temp.append(19)                    \n",
    "                else:\n",
    "                    temp.append(20)\n",
    "            if i<0:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                elif i<self.threshold[0][3]:\n",
    "                    temp.append(4)\n",
    "                elif i<self.threshold[0][4]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[0][5]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[0][6]:\n",
    "                    temp.append(7)\n",
    "                elif i<self.threshold[0][7]:\n",
    "                    temp.append(8)\n",
    "                elif i<self.threshold[0][8]:\n",
    "                    temp.append(9)                    \n",
    "                else:\n",
    "                    temp.append(10)\n",
    "        return np.asarray(temp)    \n",
    "    \n",
    "class categorise_x(): #flexible number of categories\n",
    "    \n",
    "    def __init__(self,x):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[]\n",
    "        self.num=x\n",
    "        for i in range(1,x):\n",
    "            self.percentiles.append(i*100/x)        \n",
    "            \n",
    "    def fit(self,array):\n",
    "        \n",
    "        positive=array[array>0]\n",
    "        negative=array[array<=0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "        \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for num in array:\n",
    "            if num<0:\n",
    "                counter=0\n",
    "                for i in self.threshold[0]:\n",
    "                    if num>=i:\n",
    "                        counter+=1\n",
    "                    else:\n",
    "                        break\n",
    "                temp.append(counter+1)\n",
    "            else:\n",
    "                counter=0\n",
    "                for i in self.threshold[1]:\n",
    "                    if num>=i:\n",
    "                        counter+=1\n",
    "                    else:\n",
    "                        break\n",
    "                temp.append(counter+self.num+1)\n",
    "        return np.asarray(temp)\n",
    "    \n",
    "class cross():\n",
    "    def __init__(self):\n",
    "        self.time_last_cross=0\n",
    "        self.current_sign=True\n",
    "        self.last_time=datetime(1900, 1, 1, 8, 59)\n",
    "    def get_time(self,time,price):\n",
    "        if (time-self.last_time)>timedelta(minutes=1):\n",
    "            self.last_time=time\n",
    "            self.time_last_cross=time\n",
    "            return 0\n",
    "        self.last_time=time\n",
    "        if (price>0) and self.current_sign : #if price positive and current trend is also positive\n",
    "            return (time-self.time_last_cross).total_seconds()\n",
    "        elif (price<0) and (not self.current_sign): #if price negative and current trend is negative\n",
    "            return (time-self.time_last_cross).total_seconds()\n",
    "        else: #if price positive, trend negative or price negative, trend positive\n",
    "            self.time_last_cross=time\n",
    "            self.current_sign=(price>0)\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smart_price(dataset):\n",
    "    data=dataset.copy()\n",
    "    \n",
    "    #to combat the limit up event, where price is set to 0. \n",
    "    rows=(data.loc[:,'BidPrice1']==0) #count rows of bid price equal 0\n",
    "    if (np.any(rows)): #if there is such a row\n",
    "        data.at[rows,'BidPrice1']=data.loc[rows,'AskPrice1'] #for that row, assign ask price to it\n",
    "    rows=(data.loc[:,'AskPrice1']==0) #do the same for ask price\n",
    "    if (np.any(rows)):\n",
    "        data.at[rows,'AskPrice1']=data.loc[rows,'BidPrice1'] \n",
    "        \n",
    "    data['smart_price']=data.loc[:,'BidPrice1']*data.loc[:,'AskVol1']+data.loc[:,'AskPrice1']*data.loc[:,'BidVol1']\n",
    "    data.at[:,'smart_price']=data.loc[:,'smart_price']/(data.loc[:,['BidVol1','AskVol1']].sum(axis=1))  \n",
    "    return data\n",
    "\n",
    "def calc_present_vol(dataset):\n",
    "    data=dataset.copy()\n",
    "    data['current_vol']=data.loc[:,'Volume'].diff().fillna(0)/2\n",
    "    return data\n",
    "\n",
    "def calc_future_price(dataset,time_ahead=30,time_index=44, price_col=-2):\n",
    "    data=dataset.copy()\n",
    "    future_price=[]\n",
    "    length=len(data)\n",
    "    for i in range(len(data)):\n",
    "        current_time=data[i,time_index]+timedelta(seconds=time_ahead)\n",
    "        \n",
    "        j=0 #could alternatively use 30 x 3 then search forward and backward\n",
    "        \n",
    "        #search forwards\n",
    "        while((i+j)<length and current_time>data[(i+j),time_index]):\n",
    "            j+=1\n",
    "        if (i+j)<length:\n",
    "            #if index is in the dataframe\n",
    "            future_price.append(data[(i+j),price_col]) \n",
    "        else:\n",
    "            #price ahead does not exist\n",
    "            future_price.append(np.nan) \n",
    "    future_price=np.asarray(future_price)\n",
    "    future_price=np.expand_dims(future_price,axis=1)\n",
    "    return np.concatenate((data,future_price),axis=1)\n",
    "\n",
    "def calc_edge(dataset,future_col,current_col):\n",
    "    data=dataset.copy()\n",
    "    temp=data[:,future_col]-data[:,current_col]\n",
    "    temp=np.expand_dims(temp,axis=1)\n",
    "    return np.concatenate((data,temp),axis=1)\n",
    "\n",
    "def set_index(dataset,time_index=44):\n",
    "    data=dataset.copy()\n",
    "    index=data[:,time_index].astype(int)\n",
    "    new_index=[]\n",
    "    for j in range(len(index)):\n",
    "        i=str(index[j]*1000)\n",
    "        if len(i)==11:\n",
    "            i='0'+i\n",
    "        i=i[:-10]+':'+i[-10:]\n",
    "        i=i[:-8]+':'+i[-8:]\n",
    "        i=i[:-6]+':'+i[-6:]\n",
    "        new_index.append(datetime.strptime(i,\"%H:%M:%S:%f\"))\n",
    "    data[:,time_index]=new_index\n",
    "    return data\n",
    "\n",
    "def calc_sma_fast(dataset,price_col,duration=1,time_index=44): #faster way to calculate SMA, 0.05 seconds for 5000 rows\n",
    "    data=dataset.copy()\n",
    "    sma_values=[] \n",
    "    smart_sum=np.cumsum(data[:,price_col]) #smart price column is -4\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        #finding ending point\n",
    "        last_time=data[i,time_index]-timedelta(minutes=duration)\n",
    "        \n",
    "        #finding start point\n",
    "        j=220*duration#4x60=240\n",
    "        \n",
    "        if i-j>0:\n",
    "            if data[i-j,time_index]>last_time: \n",
    "                \n",
    "                #if starting point time is greater than ending point time\n",
    "                #search backward\n",
    "                while(i-j>0 and data[i-j,time_index]>last_time):\n",
    "                    j+=1\n",
    "                    \n",
    "                #activate next line in order to debug and troubleshoot\n",
    "                #if data[i-j,time_index]!=last_time:\n",
    "                #    print('backward',i,j,data[i,time_index],data[i-j,time_index],last_time)\n",
    "\n",
    "                sma=(smart_sum[i]-smart_sum[i-j])/j\n",
    "                sma_values.append(sma)                \n",
    "                \n",
    "            else: \n",
    "                \n",
    "                #search forward\n",
    "                while(data[i-j,time_index]<last_time):\n",
    "                    j-=1\n",
    "                    \n",
    "                #activate next line in order to debug and troubleshoot\n",
    "                #if data[i-j,time_index]!=last_time:\n",
    "                #    print('forward',i,j,data[i,time_index],data[i-j,time_index],last_time)\n",
    "                \n",
    "                if j!=0:\n",
    "                    if data[i-j,time_index]!=last_time:\n",
    "                        j+=1\n",
    "                    sma=(smart_sum[i]-smart_sum[i-j])/j\n",
    "                    sma_values.append(sma)   \n",
    "                    \n",
    "                else:\n",
    "                    sma_values.append(data[i,price_col])\n",
    "                    \n",
    "        else: #starting point is at 0\n",
    "            \n",
    "            sma=smart_sum[i]/(i+1)\n",
    "            sma_values.append(sma)                       \n",
    "\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=data[:,price_col]-sma_values\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1)  \n",
    "\n",
    "def calc_volatility_slow(dataset,price_col,duration=15,time_index=44): \n",
    "    data=dataset.copy()\n",
    "    diff=np.diff(dataset[:,price_col])\n",
    "    diff=np.insert(diff,0,0)\n",
    "    \n",
    "    volatility_values=[]\n",
    "    for i in range(len(data)):\n",
    "            \n",
    "        #finding ending point\n",
    "        last_time=data[i,time_index]-timedelta(minutes=duration)\n",
    "        \n",
    "        #finding start point\n",
    "        j=220*duration#4x60=240\n",
    "        \n",
    "        if i-j>0:\n",
    "            if data[i-j,time_index]>last_time: \n",
    "                #if starting point time is greater than ending point time\n",
    "                #search backward\n",
    "                while(i-j>0 and data[i-j,time_index]>last_time):\n",
    "                    j+=1\n",
    "                vol=np.std(diff[i-j:i])\n",
    "                volatility_values.append(vol)                     \n",
    "            else: \n",
    "                #search forward\n",
    "                while(data[i-j,time_index]<last_time):\n",
    "                    j-=1\n",
    "                if j!=0:\n",
    "                    vol=np.std(diff[i-j:i])\n",
    "                    volatility_values.append(vol) \n",
    "                else:\n",
    "                    volatility_values.append(0)\n",
    "        else: #starting point is at 0\n",
    "            if i==0:\n",
    "                volatility_values.append(0)\n",
    "                continue\n",
    "            vol=np.std(diff[:i])\n",
    "            volatility_values.append(vol)   \n",
    "            \n",
    "    volatility_values=np.asarray(volatility_values)\n",
    "    volatility_values=np.expand_dims(volatility_values,axis=1)\n",
    "    return np.concatenate((data,volatility_values),axis=1)\n",
    "\n",
    "def calc_past_vol(dataset,vol_col,duration=1,time_index=44): #\n",
    "    data=dataset.copy()\n",
    "    vol_values=[] \n",
    "    vol_sum=np.cumsum(data[:,vol_col])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,time_index]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,time_index]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            vol=(vol_sum[i]-vol_sum[i-j])\n",
    "            vol_values.append(vol)\n",
    "        else:\n",
    "            vol=vol_sum[i]\n",
    "            vol_values.append(vol)\n",
    "    vol_values=np.asarray(vol_values)\n",
    "    vol_values=np.expand_dims(vol_values,axis=1)\n",
    "    return np.concatenate((data,vol_values),axis=1) \n",
    "\n",
    "def last_cross(dataset,price_col,time_index=44):\n",
    "    data=dataset.copy()\n",
    "    last_cross=cross()\n",
    "    timings=[]\n",
    "    for i in range(len(data)):\n",
    "        timings.append(last_cross.get_time(data[i,time_index],data[i,price_col]))\n",
    "    timings=np.asarray(timings)\n",
    "    timings=np.expand_dims(timings,axis=1)\n",
    "    return np.concatenate((data,timings),axis=1)\n",
    "\n",
    "def get_case(array):\n",
    "    current=1\n",
    "    previous=0\n",
    "    temp=[]\n",
    "    for i in array:\n",
    "        if i==current:\n",
    "            if i>previous:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        elif i>current:\n",
    "            previous=current\n",
    "            current=i\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            previous=current\n",
    "            current=i\n",
    "            temp.append(0)\n",
    "    return np.asarray(temp)\n",
    "\n",
    "\n",
    "def binaryToDecimal(binary): \n",
    "      \n",
    "    binary1 = binary \n",
    "    decimal, i, n = 0, 0, 0\n",
    "    while(binary != 0): \n",
    "        dec = binary % 10\n",
    "        decimal = decimal + dec * pow(2, i) \n",
    "        binary = binary//10\n",
    "        i += 1\n",
    "    return(decimal) \n",
    "    \n",
    "def get_case_n(array,n=2):\n",
    "    temp=[]\n",
    "    if n==1:\n",
    "        return(\"error\")\n",
    "    previous=np.zeros(n+1)\n",
    "    for i in array:\n",
    "        #print(i,previous)\n",
    "        final=''\n",
    "        if i==int(previous[-1]):\n",
    "            before=10\n",
    "            for j in previous:\n",
    "                if j>before:\n",
    "                    final+='1'\n",
    "                else:\n",
    "                    final+='0'\n",
    "                before=int(j)\n",
    "            final=final[1:]\n",
    "        else:\n",
    "            before=10\n",
    "            for j in previous[1:]:\n",
    "                if j>before:\n",
    "                    final+='1'\n",
    "                else:\n",
    "                    final+='0'\n",
    "                before=int(j)\n",
    "            final=final[1:]\n",
    "            if i>before:\n",
    "                final+='1'\n",
    "            else:\n",
    "                final+='0'\n",
    "        \n",
    "            previous=np.delete(previous,0)\n",
    "            previous=np.insert(previous,n,i)\n",
    "        #print(final)\n",
    "        temp.append(binaryToDecimal(int(final)))\n",
    "    return np.asarray(temp)    \n",
    "\n",
    "def process(dataset,sma_duration=1,vol_duration=1,time_index=44):\n",
    "    data=dataset.copy()\n",
    "    data=calc_smart_price(data).values #new\n",
    "    data=set_index(data,time_index=time_index) #no change\n",
    "    data=calc_future_price(data,time_index=time_index,price_col=-1) #new\n",
    "    data=calc_edge(data,future_col=-1,current_col=-2) #new\n",
    "    data=calc_sma_fast(data,duration=sma_duration,time_index=time_index,price_col=-3) #new\n",
    "    return data\n",
    "\n",
    "def process_light(dataset,sma_duration=1,vol_duration=1):\n",
    "    data=dataset.copy()\n",
    "    data=calc_smart_price(data)\n",
    "    data=data.loc[:,['Time','smart_price']].values.astype(object) #new\n",
    "    data=set_index(data,time_index=0) #no change\n",
    "    data=calc_future_price(data,time_index=0,price_col=-1) #new\n",
    "    data=calc_edge(data,future_col=-1,current_col=-2) #new\n",
    "    data=calc_sma_fast(data,duration=sma_duration,time_index=0,price_col=-3) #new\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(path,file_list,sma_duration=1):\n",
    "    df_list=[]\n",
    "    name_list=[]\n",
    "    print('processing files...')\n",
    "    for file in file_list: #read all files and add them to file_list\n",
    "        if file[-3:]=='csv': #check if file is a CSV\n",
    "            name_list.append(file)\n",
    "            df_list.append(process_light(pd.read_csv(path+file),sma_duration=sma_duration))\n",
    "            print(file)\n",
    "    print('complete processing')\n",
    "    return name_list,df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(df_path, df_list,name_list,sma_duration,n=2):\n",
    "\n",
    "    #create a list to hold all the data\n",
    "    data_list=[]\n",
    "    \n",
    "    #creating a list for each category\n",
    "    for i in range(20):\n",
    "        data_list.append([])\n",
    "        #creating a list for each quartile\n",
    "        for _ in range((2**n)):\n",
    "            data_list[i].append([])\n",
    "\n",
    "    for i in range(len(df_list)): #for each 20 day rolling window\n",
    "        if i<19: #skip first 19 days\n",
    "            continue\n",
    "        print(name_list[i])\n",
    "\n",
    "        #get -19 day\n",
    "        sma=df_list[i-19][:,-1].copy() #column for SMA\n",
    "\n",
    "        #get -18 to 0 day (19 days in total)\n",
    "        for k in range((i-18),i+1): #get 20 day moving averages\n",
    "            sma=np.concatenate((sma,df_list[k][:,-1].copy()))\n",
    "\n",
    "        cat_sma=categorise_10()\n",
    "        cat_sma.fit(sma) #calculate quartile thresholds for past 20 days\n",
    "\n",
    "        #get x,y for regression\n",
    "        x_today=df_list[i][:,-1].copy().astype(float) #column for SMA     \n",
    "        y_today=df_list[i][:,-2].copy().astype(float) #column for edge\n",
    "\n",
    "        #removing all NA\n",
    "        isnum=(~np.isnan(x_today)) & (~np.isnan(y_today))\n",
    "        x_today=x_today[isnum]\n",
    "        y_today=y_today[isnum]\n",
    "\n",
    "        #get categories of today's sma  \n",
    "        cat_x_today=cat_sma.return_quartile(x_today)\n",
    "\n",
    "        change=get_case_n(cat_x_today.copy())\n",
    "\n",
    "        #for each category\n",
    "        for cat in range(1,21):\n",
    "\n",
    "            #today's sma filter\n",
    "            sma_filter_today=(cat_x_today==cat)\n",
    "\n",
    "            #for each case\n",
    "            for case in range(2**n):\n",
    "\n",
    "                filtered= sma_filter_today & (change==case) #filtering by SMA and case\n",
    "                filtered[:2]=False #removing first 2 entries due to insufficient data\n",
    "                new_y=y_today[filtered].copy()\n",
    "\n",
    "                if (len(new_y)!=0):\n",
    "                    #add today's data into the list by category and case\n",
    "                    data_list[(cat-1)][case].append(new_y) \n",
    "\n",
    "    final_df=pd.DataFrame()                 \n",
    "    for cat in range(20):\n",
    "        for case in range(2**n):\n",
    "            reg_result={}\n",
    "            if len(data_list[cat][case])==0:\n",
    "                reg_result['category']=cat+1\n",
    "                reg_result['case']=case+1\n",
    "                reg_result['mean']=np.nan\n",
    "                reg_result['std']=np.nan\n",
    "                reg_result['num obs']=0\n",
    "                final_df=final_df.append(reg_result,ignore_index=True)             \n",
    "                continue\n",
    "\n",
    "            #getting all the data needed to calculate mean and std\n",
    "            all_data=data_list[cat][case][0].copy()\n",
    "            for i in range(1,(len(data_list[cat][case]))):\n",
    "                all_data=np.concatenate((all_data,data_list[cat][case][i].copy()))\n",
    "\n",
    "            #adding results to output\n",
    "            reg_result['category']=cat+1\n",
    "            reg_result['case']=case+1\n",
    "            reg_result['mean']=np.mean(all_data)\n",
    "            reg_result['std']=np.std(all_data)\n",
    "            reg_result['num obs']=len(all_data)\n",
    "            final_df=final_df.append(reg_result,ignore_index=True) \n",
    "\n",
    "    temp=df_path+'result_10split_'+str(2**n)+'cases_'+str(sma_duration)+'min_sma_weightedmean.csv'\n",
    "    final_df.to_csv(temp)\n",
    "    print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start /Users/hudsonyeo/Desktop/Python/leo/data/day/RM/ 1\n",
      "processing files...\n",
      "2019.01.02.csv\n",
      "2019.01.03.csv\n",
      "2019.01.04.csv\n",
      "2019.01.07.csv\n",
      "2019.01.08.csv\n",
      "2019.01.09.csv\n",
      "2019.01.10.csv\n",
      "2019.01.11.csv\n",
      "2019.01.14.csv\n",
      "2019.01.15.csv\n",
      "2019.01.16.csv\n",
      "2019.01.17.csv\n",
      "2019.01.18.csv\n",
      "2019.01.21.csv\n",
      "2019.01.22.csv\n",
      "2019.01.23.csv\n",
      "2019.01.24.csv\n",
      "2019.01.25.csv\n",
      "2019.01.28.csv\n",
      "2019.01.29.csv\n",
      "2019.01.30.csv\n",
      "2019.01.31.csv\n",
      "2019.02.01.csv\n",
      "2019.02.11.csv\n",
      "2019.02.12.csv\n",
      "2019.02.13.csv\n",
      "2019.02.14.csv\n",
      "2019.02.15.csv\n",
      "2019.02.18.csv\n",
      "2019.02.19.csv\n",
      "2019.02.20.csv\n",
      "2019.02.21.csv\n",
      "2019.02.22.csv\n",
      "2019.02.25.csv\n",
      "2019.02.26.csv\n",
      "2019.02.27.csv\n",
      "2019.02.28.csv\n",
      "2019.03.01.csv\n",
      "2019.03.04.csv\n",
      "2019.03.05.csv\n",
      "2019.03.06.csv\n",
      "2019.03.07.csv\n",
      "2019.03.08.csv\n",
      "2019.03.11.csv\n",
      "2019.03.12.csv\n",
      "2019.03.13.csv\n",
      "2019.03.14.csv\n",
      "2019.03.15.csv\n",
      "2019.03.18.csv\n",
      "2019.03.19.csv\n",
      "2019.03.20.csv\n",
      "2019.03.21.csv\n",
      "2019.03.22.csv\n",
      "2019.03.25.csv\n",
      "2019.03.26.csv\n",
      "2019.03.27.csv\n",
      "2019.03.28.csv\n",
      "2019.03.29.csv\n",
      "2019.04.01.csv\n",
      "2019.04.02.csv\n",
      "2019.04.03.csv\n",
      "2019.04.04.csv\n",
      "2019.04.08.csv\n",
      "2019.04.09.csv\n",
      "2019.04.10.csv\n",
      "2019.04.11.csv\n",
      "2019.04.12.csv\n",
      "2019.04.15.csv\n",
      "2019.04.16.csv\n",
      "2019.04.17.csv\n",
      "2019.04.18.csv\n",
      "2019.04.19.csv\n",
      "2019.04.22.csv\n",
      "2019.04.23.csv\n",
      "2019.04.24.csv\n",
      "2019.04.25.csv\n",
      "2019.04.26.csv\n",
      "2019.04.29.csv\n",
      "2019.04.30.csv\n",
      "2019.05.06.csv\n",
      "2019.05.07.csv\n",
      "2019.05.08.csv\n",
      "2019.05.09.csv\n",
      "complete processing\n",
      "287.7650649547577\n",
      "2019.01.29.csv\n",
      "2019.01.30.csv\n",
      "2019.01.31.csv\n",
      "2019.02.01.csv\n",
      "2019.02.11.csv\n",
      "2019.02.12.csv\n",
      "2019.02.13.csv\n",
      "2019.02.14.csv\n",
      "2019.02.15.csv\n",
      "2019.02.18.csv\n",
      "2019.02.19.csv\n",
      "2019.02.20.csv\n",
      "2019.02.21.csv\n",
      "2019.02.22.csv\n",
      "2019.02.25.csv\n",
      "2019.02.26.csv\n",
      "2019.02.27.csv\n",
      "2019.02.28.csv\n",
      "2019.03.01.csv\n",
      "2019.03.04.csv\n",
      "2019.03.05.csv\n",
      "2019.03.06.csv\n",
      "2019.03.07.csv\n",
      "2019.03.08.csv\n",
      "2019.03.11.csv\n",
      "2019.03.12.csv\n",
      "2019.03.13.csv\n",
      "2019.03.14.csv\n",
      "2019.03.15.csv\n",
      "2019.03.18.csv\n",
      "2019.03.19.csv\n",
      "2019.03.20.csv\n",
      "2019.03.21.csv\n",
      "2019.03.22.csv\n",
      "2019.03.25.csv\n",
      "2019.03.26.csv\n",
      "2019.03.27.csv\n",
      "2019.03.28.csv\n",
      "2019.03.29.csv\n",
      "2019.04.01.csv\n",
      "2019.04.02.csv\n",
      "2019.04.03.csv\n",
      "2019.04.04.csv\n",
      "2019.04.08.csv\n",
      "2019.04.09.csv\n",
      "2019.04.10.csv\n",
      "2019.04.11.csv\n",
      "2019.04.12.csv\n",
      "2019.04.15.csv\n",
      "2019.04.16.csv\n",
      "2019.04.17.csv\n",
      "2019.04.18.csv\n",
      "2019.04.19.csv\n",
      "2019.04.22.csv\n",
      "2019.04.23.csv\n",
      "2019.04.24.csv\n",
      "2019.04.25.csv\n",
      "2019.04.26.csv\n",
      "2019.04.29.csv\n",
      "2019.04.30.csv\n",
      "2019.05.06.csv\n",
      "2019.05.07.csv\n",
      "2019.05.08.csv\n",
      "2019.05.09.csv\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/RM//results/result_10split_4cases_1min_sma_weightedmean.csv\n",
      "complete 86.95496988296509\n",
      "start /Users/hudsonyeo/Desktop/Python/leo/data/day/RM/ 3\n",
      "processing files...\n",
      "2019.01.02.csv\n",
      "2019.01.03.csv\n",
      "2019.01.04.csv\n",
      "2019.01.07.csv\n",
      "2019.01.08.csv\n",
      "2019.01.09.csv\n",
      "2019.01.10.csv\n",
      "2019.01.11.csv\n",
      "2019.01.14.csv\n",
      "2019.01.15.csv\n",
      "2019.01.16.csv\n",
      "2019.01.17.csv\n",
      "2019.01.18.csv\n",
      "2019.01.21.csv\n",
      "2019.01.22.csv\n",
      "2019.01.23.csv\n",
      "2019.01.24.csv\n",
      "2019.01.25.csv\n",
      "2019.01.28.csv\n",
      "2019.01.29.csv\n",
      "2019.01.30.csv\n",
      "2019.01.31.csv\n",
      "2019.02.01.csv\n",
      "2019.02.11.csv\n",
      "2019.02.12.csv\n",
      "2019.02.13.csv\n",
      "2019.02.14.csv\n",
      "2019.02.15.csv\n",
      "2019.02.18.csv\n",
      "2019.02.19.csv\n",
      "2019.02.20.csv\n",
      "2019.02.21.csv\n",
      "2019.02.22.csv\n",
      "2019.02.25.csv\n"
     ]
    }
   ],
   "source": [
    "for dir_path in dir_list:\n",
    "    \n",
    "    file_list=get_file_list(dir_path)\n",
    "    file_list.sort()\n",
    "    \n",
    "    for sma_duration in [1,3,5,10]:\n",
    "        start=time.time()\n",
    "        print('start',dir_path,sma_duration) \n",
    "\n",
    "        name_list,df_list=process_files(dir_path, file_list,sma_duration=sma_duration) #process, get SMA and vol data\n",
    "        mid=time.time()\n",
    "        print(mid-start)\n",
    "        #split into categories and get means\n",
    "        run_analysis(dir_path+'/results/', df_list,name_list,sma_duration=sma_duration)  \n",
    "        \n",
    "        end=time.time()\n",
    "        print('complete',end-mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 8.4277482 ,  8.4153605 ,  8.67371353,  9.71717684,  9.85250664,\n",
       "        24.12247971, 24.65314282, 24.46319072, 24.86883893, 24.66960762,\n",
       "        23.49635926, 25.88133768, 25.25488249, 25.37282051]),\n",
       " array([25.23011217, 24.69408163, 24.03155726, 25.16373829, 24.94762684,\n",
       "        25.1653578 , 25.03897334, 24.59804433, 21.01551957, 19.40295815,\n",
       "        18.3875901 , 17.6774676 , 17.78872549, 17.9068126 , 16.71559252,\n",
       "        16.60964539, 16.13476314])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/Users/hudsonyeo/Desktop/Python/leo/data/day/RM/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/SR/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/c/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/fu/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/m/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/ni/results/result_10sma,6vol_1min,',\n",
       "  '/Users/hudsonyeo/Desktop/Python/leo/data/day/zn/results/result_10sma,6vol_1min,'],\n",
       " ['RM', 'SR', 'TA', 'c', 'fu', 'm', 'ni', 'zn'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path='/Users/hudsonyeo/Desktop/Python/leo/data/day'\n",
    "dir_list_names=os.listdir(dir_path)\n",
    "dir_list_names.sort()\n",
    "dir_list=process_dir(dir_list_names)\n",
    "#dir_list=dir_list[4:]\n",
    "for i in range(len(dir_list)):\n",
    "    dir_list[i]+='results/result_10sma,6vol_1min,'\n",
    "dir_list,dir_list_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dir_list_names[1:]:\n",
    "    for num in [1,3,5,10]:\n",
    "        path='/Users/hudsonyeo/Desktop/Python/leo/data/day/'+i+'/results/result_10sma,6vol_1min,'+str(num)+'min_sma,vol_mean.csv'\n",
    "        print(i)\n",
    "        print('sma:',num)\n",
    "        data=pd.read_csv(path)\n",
    "        x=[]\n",
    "        x_std=[]\n",
    "        pos=[]\n",
    "        num_obs=[]\n",
    "        for ma_cat in range(1,21):\n",
    "            for vol_cat in range(1,5):\n",
    "                col='ma_cat_'+str(ma_cat)+'_vol_quartile'+str(vol_cat)+'_mean'\n",
    "                std=data.loc[:,col].std()\n",
    "                mean=data.loc[:,col].mean()\n",
    "                column=data.loc[:,col]\n",
    "                x_std.append(std)\n",
    "                x.append(mean)\n",
    "                percentage=len(column[column>=0])/len(column)\n",
    "                pos.append(percentage)  \n",
    "\n",
    "                obs='ma_cat_'+str(ma_cat)+'_vol_quartile'+str(vol_cat)+'_num_obs'\n",
    "                obs=data.loc[:,obs].sum()\n",
    "                num_obs.append(obs)\n",
    "        x=np.asarray(x)\n",
    "        x=np.reshape(x,(20,4))\n",
    "        x=np.around(x,3)\n",
    "        x_std=np.asarray(x_std)\n",
    "        x_std=np.reshape(x_std,(20,4))\n",
    "        pos=np.asarray(pos)\n",
    "        pos=np.reshape(pos,(20,4))\n",
    "        num_obs=np.asarray(num_obs)\n",
    "        num_obs=np.reshape(num_obs,(20,4))\n",
    "        print('mean\\n',x)\n",
    "        print('std\\n',x_std)\n",
    "        print('% positive\\n',pos)\n",
    "        print('num obs\\n',num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      " [[ 0.194  0.183  0.285  0.565]\n",
      " [ 0.235  0.03  -0.014 -0.072]\n",
      " [ 0.103  0.057  0.004 -0.077]\n",
      " [ 0.029 -0.003 -0.047  0.091]\n",
      " [-0.068  0.03   0.095  0.012]\n",
      " [-0.017 -0.05  -0.058 -0.019]\n",
      " [-0.085  0.016 -0.073  0.021]\n",
      " [-0.029 -0.024 -0.009 -0.034]\n",
      " [ 0.001  0.073 -0.04  -0.021]\n",
      " [ 0.1    0.006 -0.02  -0.004]\n",
      " [ 0.24  -0.029  0.041  0.004]\n",
      " [ 0.151  0.08   0.008 -0.105]\n",
      " [ 0.024  0.087  0.046 -0.082]\n",
      " [ 0.08   0.023 -0.065 -0.166]\n",
      " [ 0.018  0.094  0.01  -0.024]\n",
      " [ 0.02  -0.014 -0.001 -0.004]\n",
      " [ 0.084  0.038 -0.105 -0.023]\n",
      " [-0.006  0.066 -0.001 -0.17 ]\n",
      " [ 0.125  0.058 -0.01   0.477]\n",
      " [-0.257  0.311  0.11  -0.564]]\n",
      "std\n",
      " [[0.93956164 0.87898641 0.78070106 0.88553184]\n",
      " [0.51811005 0.47132081 0.66336174 0.8391233 ]\n",
      " [0.48218976 0.58471722 0.41383789 0.89423982]\n",
      " [0.41440251 0.46260968 0.42024159 0.7358212 ]\n",
      " [0.40166118 0.35035442 0.34101024 0.5733473 ]\n",
      " [0.37502983 0.42876114 0.44256151 0.58864317]\n",
      " [0.33205531 0.2972276  0.41919198 0.56233476]\n",
      " [0.31447083 0.42836695 0.38497326 0.53254616]\n",
      " [0.32140864 0.93195165 0.29615264 0.4770329 ]\n",
      " [0.87947204 0.34252593 0.33061062 0.3869749 ]\n",
      " [1.24789125 0.25154782 0.286156   0.39344438]\n",
      " [0.81409619 0.36965098 0.32518716 0.50982716]\n",
      " [0.32292507 0.50143075 0.36383538 0.41361763]\n",
      " [0.35536811 0.33326876 0.3822987  0.45767989]\n",
      " [0.35647843 0.3834409  0.31413098 0.59775416]\n",
      " [0.38429082 0.37711727 0.36771527 0.50400627]\n",
      " [0.71760314 0.50175859 0.3932777  0.60730226]\n",
      " [0.63488779 0.44949551 0.63595356 0.93506509]\n",
      " [1.23862812 0.79373922 0.74624292 3.21464571]\n",
      " [0.66689917 3.24391993 2.98643079 1.35015875]]\n",
      "% positive\n",
      " [[0.71875  0.609375 0.546875 0.453125]\n",
      " [0.6875   0.515625 0.53125  0.34375 ]\n",
      " [0.53125  0.578125 0.546875 0.40625 ]\n",
      " [0.546875 0.5625   0.421875 0.5     ]\n",
      " [0.46875  0.53125  0.65625  0.46875 ]\n",
      " [0.5      0.46875  0.5      0.53125 ]\n",
      " [0.40625  0.578125 0.453125 0.53125 ]\n",
      " [0.421875 0.515625 0.40625  0.40625 ]\n",
      " [0.515625 0.46875  0.359375 0.5     ]\n",
      " [0.4375   0.578125 0.5625   0.515625]\n",
      " [0.515625 0.46875  0.578125 0.5625  ]\n",
      " [0.484375 0.5625   0.5      0.421875]\n",
      " [0.609375 0.53125  0.546875 0.421875]\n",
      " [0.59375  0.515625 0.40625  0.265625]\n",
      " [0.515625 0.53125  0.484375 0.515625]\n",
      " [0.5625   0.453125 0.515625 0.5     ]\n",
      " [0.515625 0.546875 0.453125 0.515625]\n",
      " [0.4375   0.515625 0.34375  0.5     ]\n",
      " [0.34375  0.375    0.390625 0.5625  ]\n",
      " [0.34375  0.4375   0.375    0.3125  ]]\n",
      "num obs\n",
      " [[23114. 19504. 17832. 17529.]\n",
      " [23726. 22642. 17494. 16386.]\n",
      " [24410. 20452. 18789. 15788.]\n",
      " [23311. 20414. 17305. 15711.]\n",
      " [25790. 20259. 18089. 16263.]\n",
      " [24904. 20130. 19019. 16842.]\n",
      " [25572. 20777. 19473. 16693.]\n",
      " [25543. 22009. 18389. 17019.]\n",
      " [24526. 21845. 18025. 17007.]\n",
      " [25690. 20839. 17421. 16991.]\n",
      " [24859. 21743. 17797. 17141.]\n",
      " [25415. 20739. 18112. 16317.]\n",
      " [24714. 20736. 19089. 16900.]\n",
      " [25364. 19867. 18239. 16369.]\n",
      " [25054. 20491. 18390. 16556.]\n",
      " [23794. 20177. 18575. 16723.]\n",
      " [24198. 19965. 17875. 16258.]\n",
      " [24291. 18791. 19021. 16720.]\n",
      " [25087. 21074. 18773. 16784.]\n",
      " [24152. 20299. 18754. 18731.]]\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/fu/results/second/result_10sma,6vol_1min,5min_sma,vol_mean.csv')\n",
    "x=[]\n",
    "x_std=[]\n",
    "pos=[]\n",
    "num_obs=[]\n",
    "for ma_cat in range(1,21):\n",
    "    for vol_cat in range(1,5):\n",
    "        col='ma_cat_'+str(ma_cat)+'_vol_quartile'+str(vol_cat)+'_mean'\n",
    "        std=data.loc[:,col].std()\n",
    "        mean=data.loc[:,col].mean()\n",
    "        column=data.loc[:,col]\n",
    "        x_std.append(std)\n",
    "        x.append(mean)\n",
    "        percentage=len(column[column>=0])/len(column)\n",
    "        pos.append(percentage)  \n",
    "        \n",
    "        obs='ma_cat_'+str(ma_cat)+'_vol_quartile'+str(vol_cat)+'_num_obs'\n",
    "        obs=data.loc[:,obs].sum()\n",
    "        num_obs.append(obs)\n",
    "x=np.asarray(x)\n",
    "x=np.reshape(x,(20,4))\n",
    "x=np.around(x,3)\n",
    "x_std=np.asarray(x_std)\n",
    "x_std=np.reshape(x_std,(20,4))\n",
    "pos=np.asarray(pos)\n",
    "pos=np.reshape(pos,(20,4))\n",
    "num_obs=np.asarray(num_obs)\n",
    "num_obs=np.reshape(num_obs,(20,4))\n",
    "print('mean\\n',x)\n",
    "print('std\\n',x_std)\n",
    "print('% positive\\n',pos)\n",
    "print('num obs\\n',num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
