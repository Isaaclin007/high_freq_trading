{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "os.chdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list=os.listdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '2019.01.02.csv',\n",
       " '2019.01.03.csv',\n",
       " '2019.01.04.csv',\n",
       " '2019.01.07.csv',\n",
       " '2019.01.08.csv',\n",
       " '2019.01.09.csv',\n",
       " '2019.01.10.csv',\n",
       " '2019.01.11.csv',\n",
       " '2019.01.14.csv',\n",
       " '2019.01.15.csv',\n",
       " '2019.01.16.csv',\n",
       " '2019.01.17.csv',\n",
       " '2019.01.18.csv',\n",
       " '2019.01.21.csv',\n",
       " '2019.01.22.csv',\n",
       " '2019.01.23.csv',\n",
       " '2019.01.24.csv',\n",
       " '2019.01.25.csv',\n",
       " '2019.01.28.csv',\n",
       " '2019.01.29.csv',\n",
       " '2019.01.30.csv',\n",
       " '2019.01.31.csv',\n",
       " '2019.02.01.csv',\n",
       " '2019.02.11.csv',\n",
       " '2019.02.12.csv',\n",
       " '2019.02.13.csv',\n",
       " '2019.02.14.csv',\n",
       " '2019.02.15.csv',\n",
       " '2019.02.18.csv',\n",
       " '2019.02.19.csv',\n",
       " '2019.02.20.csv',\n",
       " '2019.02.21.csv',\n",
       " '2019.02.22.csv',\n",
       " '2019.02.25.csv',\n",
       " '2019.02.26.csv',\n",
       " '2019.02.27.csv',\n",
       " '2019.02.28.csv',\n",
       " '2019.03.01.csv',\n",
       " '2019.03.04.csv',\n",
       " '2019.03.05.csv',\n",
       " '2019.03.06.csv',\n",
       " '2019.03.07.csv',\n",
       " '2019.03.08.csv',\n",
       " '2019.03.11.csv',\n",
       " '2019.03.12.csv',\n",
       " '2019.03.13.csv',\n",
       " '2019.03.14.csv',\n",
       " '2019.03.15.csv',\n",
       " '2019.03.18.csv',\n",
       " '2019.03.19.csv',\n",
       " '2019.03.20.csv',\n",
       " '2019.03.21.csv',\n",
       " '2019.03.22.csv',\n",
       " '2019.03.25.csv',\n",
       " '2019.03.26.csv',\n",
       " '2019.03.27.csv',\n",
       " '2019.03.28.csv',\n",
       " '2019.03.29.csv',\n",
       " '2019.04.01.csv',\n",
       " '2019.04.02.csv',\n",
       " '2019.04.03.csv',\n",
       " '2019.04.04.csv',\n",
       " '2019.04.08.csv',\n",
       " '2019.04.09.csv',\n",
       " '2019.04.10.csv',\n",
       " '2019.04.11.csv',\n",
       " '2019.04.12.csv',\n",
       " '2019.04.15.csv',\n",
       " '2019.04.16.csv',\n",
       " '2019.04.17.csv',\n",
       " '2019.04.18.csv',\n",
       " '2019.04.19.csv',\n",
       " '2019.04.22.csv',\n",
       " '2019.04.23.csv',\n",
       " '2019.04.24.csv',\n",
       " '2019.04.25.csv',\n",
       " '2019.04.26.csv',\n",
       " '2019.04.29.csv',\n",
       " '2019.04.30.csv',\n",
       " '2019.05.06.csv',\n",
       " '2019.05.07.csv',\n",
       " '2019.05.08.csv',\n",
       " '2019.05.09.csv',\n",
       " 'results']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorise():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(7)\n",
    "                else:\n",
    "                    temp.append(8)\n",
    "            if i<0:\n",
    "                if i>self.threshold[0][2]:\n",
    "                    temp.append(4)\n",
    "                elif i>self.threshold[0][1]:\n",
    "                    temp.append(3)\n",
    "                elif i>self.threshold[0][0]:\n",
    "                    temp.append(2)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "        return np.asarray(temp)\n",
    "    \n",
    "class LinearRegression(linear_model.LinearRegression):\n",
    "    \"\"\"\n",
    "    LinearRegression class after sklearn's, but calculate t-statistics\n",
    "    and p-values for model coefficients (betas).\n",
    "    Additional attributes available after .fit()\n",
    "    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n",
    "    which is (n_features, n_coefs)\n",
    "    This class sets the intercept to 0 by default, since usually we include it\n",
    "    in X.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
    "                     n_jobs=1):\n",
    "            self.fit_intercept = fit_intercept\n",
    "            self.normalize = normalize\n",
    "            self.copy_X = copy_X\n",
    "            self.n_jobs = n_jobs\n",
    "    def fit(self, X, y, n_jobs=1):\n",
    "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
    "\n",
    "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
    "        se = np.array([\n",
    "            np.sqrt(np.diagonal(sse[i] * np.linalg.inv(np.dot(X.T, X))))\n",
    "                                                    for i in range(sse.shape[0])\n",
    "                    ])\n",
    "\n",
    "        self.t = self.coef_ / se\n",
    "        self.p = 2 * (1 - stats.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1]))\n",
    "        return self                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def calc_vwap(dataset,duration=1): #to be implement\\n    data=dataset[:]\\n    for i in data[:,44]:\\n        last_time=i-timedelta(minutes=duration)\\n        rolling=data[(data[:,44]>=last_time) & (data[:,44]<i)]\\n        high=rolling[:,51].max()\\n        low=rolling[:,51].min()\\n        avg=(rolling[-1,51]+high+low)/3\\ndef calc_rsi(dataset)        '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_smart_price(dataset):\n",
    "    data=dataset[:]\n",
    "    \n",
    "    #to combat the limit up event, where price is set to 0. \n",
    "    rows=(data.loc[:,'BidPrice1']==0) #count rows of bid price equal 0\n",
    "    if (np.any(rows)): #if there is such a row\n",
    "        data.at[rows,'BidPrice1']=data.loc[rows,'AskPrice1'] #for that row, assign ask price to it\n",
    "    rows=(data.loc[:,'AskPrice1']==0) #do the same for ask price\n",
    "    if (np.any(rows)):\n",
    "        data.at[rows,'AskPrice1']=data.loc[rows,'BidPrice1'] \n",
    "        \n",
    "    data['smart_price']=data.loc[:,'BidPrice1']*data.loc[:,'AskVol1']+data.loc[:,'AskPrice1']*data.loc[:,'BidVol1']\n",
    "    data.at[:,'smart_price']=data.loc[:,'smart_price']/(data.loc[:,['BidVol1','AskVol1']].sum(axis=1))  \n",
    "    return data\n",
    "\n",
    "def calc_future_price(dataset,time_ahead=30):\n",
    "    data=dataset[:]\n",
    "    future_price=[]\n",
    "    length=len(data)\n",
    "    for i in range(len(data)):\n",
    "        current_time=data[i,44]+timedelta(seconds=time_ahead)\n",
    "        #print(data[i,44])\n",
    "        j=0\n",
    "        #print(current_time)\n",
    "        while((i+j)<length and current_time>data[(i+j),44]):\n",
    "            j+=1\n",
    "        #print(i,j,(data[(i+j-1),44]))\n",
    "        if (i+j)<length:\n",
    "            future_price.append(data[(i+j),51]) #51 is the index for smart price            \n",
    "        else:\n",
    "            future_price.append(np.nan)\n",
    "    future_price=np.asarray(future_price)\n",
    "    future_price=np.expand_dims(future_price,axis=1)\n",
    "    return np.concatenate((data,future_price),axis=1)\n",
    "\n",
    "\n",
    "def calc_edge(dataset):\n",
    "    data=dataset.copy()\n",
    "    temp=data[:,52]-data[:,51]\n",
    "    temp=np.expand_dims(temp,axis=1)\n",
    "    return np.concatenate((data,temp),axis=1)\n",
    "\n",
    "def set_index(dataset):\n",
    "    data=dataset[:]\n",
    "    index=data[:,44]\n",
    "    new_index=[]\n",
    "    for j in range(len(index)):\n",
    "        i=str(index[j]*1000)\n",
    "        if len(i)==11:\n",
    "            i='0'+i\n",
    "        i=i[:-10]+':'+i[-10:]\n",
    "        i=i[:-8]+':'+i[-8:]\n",
    "        i=i[:-6]+':'+i[-6:]\n",
    "        new_index.append(datetime.strptime(i,\"%H:%M:%S:%f\"))\n",
    "    data[:,44]=new_index\n",
    "    return data\n",
    "\n",
    "def calc_sma_fast(dataset,duration=1): #faster way to calculate SMA, 0.05 seconds for 5000 rows\n",
    "    data=dataset[:]\n",
    "    sma_values=[] \n",
    "    smart_sum=np.cumsum(data[:,51])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            sma=(smart_sum[i]-smart_sum[i-j])/(j)\n",
    "            sma_values.append(sma)\n",
    "        else:\n",
    "            sma=smart_sum[i]/(i+1)\n",
    "            sma_values.append(sma)\n",
    "\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=data[:,51]-sma_values\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1)     \n",
    "\n",
    "\n",
    "def process(dataset):\n",
    "    data=dataset[:]\n",
    "    data=calc_smart_price(data).values #51\n",
    "    data=set_index(data)\n",
    "    data=calc_future_price(data) #52\n",
    "    data=calc_edge(data) #53\n",
    "    data=calc_sma_fast(data,duration=1) #54\n",
    "    return data\n",
    "ma_dict={'-4':'1',\n",
    "        '-3':'5',\n",
    "        '-2':'15',\n",
    "        '-1':'30',\n",
    "        '54':'1',\n",
    "        '55':'5',\n",
    "        '56':'15',\n",
    "        '57':'30'}    \n",
    "#ignore below        \n",
    "'''def calc_vwap(dataset,duration=1): #to be implement\n",
    "    data=dataset[:]\n",
    "    for i in data[:,44]:\n",
    "        last_time=i-timedelta(minutes=duration)\n",
    "        rolling=data[(data[:,44]>=last_time) & (data[:,44]<i)]\n",
    "        high=rolling[:,51].max()\n",
    "        low=rolling[:,51].min()\n",
    "        avg=(rolling[-1,51]+high+low)/3\n",
    "def calc_rsi(dataset)        '''     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.02.csv read\n",
      "2019.01.03.csv read\n",
      "2019.01.04.csv read\n",
      "2019.01.07.csv read\n",
      "2019.01.08.csv read\n",
      "2019.01.09.csv read\n",
      "2019.01.10.csv read\n",
      "2019.01.11.csv read\n",
      "2019.01.14.csv read\n",
      "2019.01.15.csv read\n",
      "2019.01.16.csv read\n",
      "2019.01.17.csv read\n",
      "2019.01.18.csv read\n",
      "2019.01.21.csv read\n",
      "2019.01.22.csv read\n",
      "2019.01.23.csv read\n",
      "2019.01.24.csv read\n",
      "2019.01.25.csv read\n",
      "2019.01.28.csv read\n",
      "2019.01.29.csv read\n",
      "2019.01.30.csv read\n",
      "2019.01.31.csv read\n",
      "2019.02.01.csv read\n",
      "2019.02.11.csv read\n",
      "2019.02.12.csv read\n",
      "2019.02.13.csv read\n",
      "2019.02.14.csv read\n",
      "2019.02.15.csv read\n",
      "2019.02.18.csv read\n",
      "2019.02.19.csv read\n",
      "2019.02.20.csv read\n",
      "2019.02.21.csv read\n",
      "2019.02.22.csv read\n",
      "2019.02.25.csv read\n",
      "2019.02.26.csv read\n",
      "2019.02.27.csv read\n",
      "2019.02.28.csv read\n",
      "2019.03.01.csv read\n",
      "2019.03.04.csv read\n",
      "2019.03.05.csv read\n",
      "2019.03.06.csv read\n",
      "2019.03.07.csv read\n",
      "2019.03.08.csv read\n",
      "2019.03.11.csv read\n",
      "2019.03.12.csv read\n",
      "2019.03.13.csv read\n",
      "2019.03.14.csv read\n",
      "2019.03.15.csv read\n",
      "2019.03.18.csv read\n",
      "2019.03.19.csv read\n",
      "2019.03.20.csv read\n",
      "2019.03.21.csv read\n",
      "2019.03.22.csv read\n",
      "2019.03.25.csv read\n",
      "2019.03.26.csv read\n",
      "2019.03.27.csv read\n",
      "2019.03.28.csv read\n",
      "2019.03.29.csv read\n",
      "2019.04.01.csv read\n",
      "2019.04.02.csv read\n",
      "2019.04.03.csv read\n",
      "2019.04.04.csv read\n",
      "2019.04.08.csv read\n",
      "2019.04.09.csv read\n",
      "2019.04.10.csv read\n",
      "2019.04.11.csv read\n",
      "2019.04.12.csv read\n",
      "2019.04.15.csv read\n",
      "2019.04.16.csv read\n",
      "2019.04.17.csv read\n",
      "2019.04.18.csv read\n",
      "2019.04.19.csv read\n",
      "2019.04.22.csv read\n",
      "2019.04.23.csv read\n",
      "2019.04.24.csv read\n",
      "2019.04.25.csv read\n",
      "2019.04.26.csv read\n",
      "2019.04.29.csv read\n",
      "2019.04.30.csv read\n",
      "2019.05.06.csv read\n",
      "2019.05.07.csv read\n",
      "2019.05.08.csv read\n",
      "2019.05.09.csv read\n"
     ]
    }
   ],
   "source": [
    "file_list[1][-3:]\n",
    "df_list=[]\n",
    "name_list=[]\n",
    "path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/'\n",
    "for file in file_list: #read all files and add them to file_list\n",
    "    if file[-3:]=='csv': #check if file is a CSV\n",
    "        name_list.append(file)\n",
    "        df_list.append(process(pd.read_csv(path+file)))\n",
    "        print(file,'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.29.csv\n",
      "2019.01.30.csv\n",
      "2019.01.31.csv\n",
      "2019.02.01.csv\n",
      "2019.02.11.csv\n",
      "2019.02.12.csv\n",
      "2019.02.13.csv\n",
      "2019.02.14.csv\n",
      "2019.02.15.csv\n",
      "2019.02.18.csv\n",
      "2019.02.19.csv\n",
      "2019.02.20.csv\n",
      "2019.02.21.csv\n",
      "2019.02.22.csv\n",
      "2019.02.25.csv\n",
      "2019.02.26.csv\n",
      "2019.02.27.csv\n",
      "2019.02.28.csv\n",
      "2019.03.01.csv\n",
      "2019.03.04.csv\n",
      "2019.03.05.csv\n",
      "2019.03.06.csv\n",
      "2019.03.07.csv\n",
      "2019.03.08.csv\n",
      "2019.03.11.csv\n",
      "2019.03.12.csv\n",
      "2019.03.13.csv\n",
      "2019.03.14.csv\n",
      "2019.03.15.csv\n",
      "2019.03.18.csv\n",
      "2019.03.19.csv\n",
      "2019.03.20.csv\n",
      "2019.03.21.csv\n",
      "2019.03.22.csv\n",
      "2019.03.25.csv\n",
      "2019.03.26.csv\n",
      "2019.03.27.csv\n",
      "2019.03.28.csv\n",
      "2019.03.29.csv\n",
      "2019.04.01.csv\n",
      "2019.04.02.csv\n",
      "2019.04.03.csv\n",
      "2019.04.04.csv\n",
      "2019.04.08.csv\n",
      "2019.04.09.csv\n",
      "2019.04.10.csv\n",
      "2019.04.11.csv\n",
      "2019.04.12.csv\n",
      "2019.04.15.csv\n",
      "2019.04.16.csv\n",
      "2019.04.17.csv\n",
      "2019.04.18.csv\n",
      "2019.04.19.csv\n",
      "2019.04.22.csv\n",
      "2019.04.23.csv\n",
      "2019.04.24.csv\n",
      "2019.04.25.csv\n",
      "2019.04.26.csv\n",
      "2019.04.29.csv\n",
      "2019.04.30.csv\n",
      "2019.05.06.csv\n",
      "2019.05.07.csv\n",
      "2019.05.08.csv\n",
      "2019.05.09.csv\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_binary_1day_8split_1minute_ma_nointercept.csv\n"
     ]
    }
   ],
   "source": [
    "#run regressions against all 4 moving averages\n",
    "df_path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/'\n",
    "\n",
    "final_df=pd.DataFrame()    \n",
    "\n",
    "for i in range(len(df_list)): #for each 20 day rolling window\n",
    "    if i<19:\n",
    "        continue\n",
    "    print(name_list[i])\n",
    "\n",
    "    #get -19 day\n",
    "    x=df_list[i-19][:,54]\n",
    "    #get -18 to 0 day (19 days in total)\n",
    "    for k in range((i-18),i+1): #get 20 day moving averages\n",
    "        x=np.concatenate((x,df_list[k][:,54]))\n",
    "\n",
    "    cat=categorise()\n",
    "    cat.fit(x) #calculate quartile thresholds for past 20 days\n",
    "\n",
    "    #get x,y for regression\n",
    "    x=df_list[i][:,54] #column for SMA\n",
    "    x=x.astype(float)        \n",
    "    y=df_list[i][:,53] #column for edge\n",
    "    y=y.astype(float)\n",
    "\n",
    "    #removing all NA\n",
    "    isnum=(~np.isnan(x)) & (~np.isnan(y))\n",
    "    y=y[isnum]\n",
    "    x=x[isnum]        \n",
    "\n",
    "    #get the quartiles of the SMA\n",
    "    category=cat.return_quartile(x)\n",
    "\n",
    "    reg_result={}\n",
    "    reg_result['date']=name_list[i]\n",
    "    reg_result['total_obs']=len(x)\n",
    "\n",
    "    for quartile in range(1,9):\n",
    "\n",
    "        #filter by quartile\n",
    "        filtered=(category==quartile) \n",
    "        q='quartile_'+str(quartile)+'_'\n",
    "        if (np.any(filtered)):\n",
    "            new_x=x[filtered]\n",
    "            new_y=y[filtered] \n",
    "            new_x=np.expand_dims(new_x,1)\n",
    "            new_y=np.expand_dims(new_y,1)\n",
    "            lin_model=LinearRegression(fit_intercept=False)\n",
    "            lin_model.fit(new_x,new_y)\n",
    "\n",
    "            reg_result[(q+'slope')]=lin_model.coef_[0][0]\n",
    "\n",
    "            reg_result[(q+'p_val')]=lin_model.p[0][0]\n",
    "            reg_result[(q+'x_mean')]=np.mean(new_x)\n",
    "            reg_result[(q+'x_std')]=np.std(new_x)\n",
    "            reg_result[(q+'num_obs')]=len(new_x)\n",
    "\n",
    "        else:\n",
    "            reg_result[(q+'slope')]='na'\n",
    "\n",
    "            reg_result[(q+'p_val')]='na'\n",
    "            reg_result[(q+'x_mean')]='na'\n",
    "            reg_result[(q+'x_std')]='na'\n",
    "            reg_result[(q+'num_obs')]='0'\n",
    "\n",
    "    final_df=final_df.append(reg_result,ignore_index=True)                \n",
    "temp=df_path+'result_binary_1day_8split_1minute_ma_nointercept.csv'\n",
    "final_df.to_csv(temp)\n",
    "print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_binary_1day_8split_1minute_ma_nointercept.csv')\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "for i in range(1,9):\n",
    "    col='quartile_'+str(i)+'_slope'\n",
    "    row=data.loc[:,col]\n",
    "    x=[]\n",
    "    for j in row:\n",
    "        if is_number(j) and j==j:\n",
    "            k=float(j)\n",
    "            #print(k)\n",
    "            x.append(k)\n",
    "\n",
    "            \n",
    "    x=np.asarray(x)\n",
    "    pos=x[x>0]\n",
    "    perc=len(pos)/len(x)\n",
    "    print('category:',i,'percentage positive:',round(perc,7),'mean slope:',round(row.mean(),7),'std dev',round(row.std(),7))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019-01-16', 1547600399898654, 'TA905', 0, 6044, 6118, 1115984, 0,\n",
       "       0, 0, 6120, 6118, 6114, 6112, 6110, 6108, 6120, 6122, 6124, 6126,\n",
       "       6128, 10, 101, 138, 343, 178, 68, 247, 157, 205, 554, 640270,\n",
       "       19575647220, 1122674, 0, 6116, 6116, 0, 0, 6348, 5740, 32486,\n",
       "       21886, 20190116, datetime.datetime(1900, 1, 1, 9, 0), 20190116, 0,\n",
       "       32400000, 20190116, 5932, 6184, 6118.25641025641,\n",
       "       6120.674418604651, 2.418008348240619, 0.0], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.29.csv\n",
      "2019.01.30.csv\n",
      "2019.01.31.csv\n",
      "2019.02.01.csv\n",
      "2019.02.11.csv\n",
      "2019.02.12.csv\n",
      "2019.02.13.csv\n",
      "2019.02.14.csv\n",
      "2019.02.15.csv\n",
      "2019.02.18.csv\n",
      "2019.02.19.csv\n",
      "2019.02.20.csv\n",
      "2019.02.21.csv\n",
      "2019.02.22.csv\n",
      "2019.02.25.csv\n",
      "2019.02.26.csv\n",
      "2019.02.27.csv\n",
      "2019.02.28.csv\n",
      "2019.03.01.csv\n",
      "2019.03.04.csv\n",
      "2019.03.05.csv\n",
      "2019.03.06.csv\n",
      "2019.03.07.csv\n",
      "2019.03.08.csv\n",
      "2019.03.11.csv\n",
      "2019.03.12.csv\n",
      "2019.03.13.csv\n",
      "2019.03.14.csv\n",
      "2019.03.15.csv\n",
      "2019.03.18.csv\n",
      "2019.03.19.csv\n",
      "2019.03.20.csv\n",
      "2019.03.21.csv\n",
      "2019.03.22.csv\n",
      "2019.03.25.csv\n",
      "2019.03.26.csv\n",
      "2019.03.27.csv\n",
      "2019.03.28.csv\n",
      "2019.03.29.csv\n",
      "2019.04.01.csv\n",
      "2019.04.02.csv\n",
      "2019.04.03.csv\n",
      "2019.04.04.csv\n",
      "2019.04.08.csv\n",
      "2019.04.09.csv\n",
      "2019.04.10.csv\n",
      "2019.04.11.csv\n",
      "2019.04.12.csv\n",
      "2019.04.15.csv\n",
      "2019.04.16.csv\n",
      "2019.04.17.csv\n",
      "2019.04.18.csv\n",
      "2019.04.19.csv\n",
      "2019.04.22.csv\n",
      "2019.04.23.csv\n",
      "2019.04.24.csv\n",
      "2019.04.25.csv\n",
      "2019.04.26.csv\n",
      "2019.04.29.csv\n",
      "2019.04.30.csv\n",
      "2019.05.06.csv\n",
      "2019.05.07.csv\n",
      "2019.05.08.csv\n",
      "2019.05.09.csv\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_binary_1day_8split_1minute_ma_nointercept.csv\n"
     ]
    }
   ],
   "source": [
    "##binary\n",
    "#run regressions against all 4 moving averages\n",
    "df_path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/'\n",
    "\n",
    "final_df=pd.DataFrame()    \n",
    "\n",
    "for i in range(len(df_list)): #for each 20 day rolling window\n",
    "    if i<19:\n",
    "        continue\n",
    "    print(name_list[i])\n",
    "\n",
    "    #get -19 day\n",
    "    x=df_list[i-19][:,54]\n",
    "    #get -18 to 0 day (19 days in total)\n",
    "    for k in range((i-18),i+1): #get 20 day moving averages\n",
    "        x=np.concatenate((x,df_list[k][:,54]))\n",
    "\n",
    "    cat=categorise()\n",
    "    cat.fit(x) #calculate quartile thresholds for past 20 days\n",
    "\n",
    "    #get x,y for regression\n",
    "    x=df_list[i][:,54] #column for SMA\n",
    "    x=x.astype(float)        \n",
    "    y=df_list[i][:,53] #column for edge\n",
    "    y=y.astype(float)\n",
    "\n",
    "    #removing all NA\n",
    "    isnum=(~np.isnan(x)) & (~np.isnan(y))\n",
    "    y=y[isnum]\n",
    "    x=x[isnum]        \n",
    "\n",
    "    #get the quartiles of the SMA\n",
    "    category=cat.return_quartile(x)\n",
    "\n",
    "    reg_result={}\n",
    "    reg_result['date']=name_list[i]\n",
    "    reg_result['total_obs']=len(x)\n",
    "\n",
    "    for quartile in range(1,9):\n",
    "\n",
    "        #filter by quartile\n",
    "        filtered=(category==quartile) \n",
    "        q='quartile_'+str(quartile)+'_'\n",
    "        if (np.any(filtered)):\n",
    "            new_x=x[filtered]\n",
    "            new_x[new_x>0]=1\n",
    "            new_x[new_x<0]=-1\n",
    "            new_y=y[filtered] \n",
    "            new_x=np.expand_dims(new_x,1)\n",
    "            new_y=np.expand_dims(new_y,1)\n",
    "            lin_model=LinearRegression(fit_intercept=False)\n",
    "            lin_model.fit(new_x,new_y)\n",
    "\n",
    "            reg_result[(q+'slope')]=lin_model.coef_[0][0]\n",
    "\n",
    "            reg_result[(q+'p_val')]=lin_model.p[0][0]\n",
    "            reg_result[(q+'x_mean')]=np.mean(new_x)\n",
    "            reg_result[(q+'x_std')]=np.std(new_x)\n",
    "            reg_result[(q+'num_obs')]=len(new_x)\n",
    "\n",
    "        else:\n",
    "            reg_result[(q+'slope')]='na'\n",
    "\n",
    "            reg_result[(q+'p_val')]='na'\n",
    "            reg_result[(q+'x_mean')]='na'\n",
    "            reg_result[(q+'x_std')]='na'\n",
    "            reg_result[(q+'num_obs')]='0'\n",
    "\n",
    "    final_df=final_df.append(reg_result,ignore_index=True)                \n",
    "temp=df_path+'result_binary_1day_8split_1minute_ma_nointercept.csv'\n",
    "final_df.to_csv(temp)\n",
    "print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
