{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into 20 categories (10-90% percentile for positive and negative so 10x2) based on SMA 1 minute from past 20 days, \n",
    "#then within each SMA category, separate into quartiles based on cumulative volume in 1 minute\n",
    "#calculate the mean and std of the edge in each case\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA'\n",
    "os.chdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list=os.listdir(path)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '2019.01.02.csv',\n",
       " '2019.01.03.csv',\n",
       " '2019.01.04.csv',\n",
       " '2019.01.07.csv',\n",
       " '2019.01.08.csv',\n",
       " '2019.01.09.csv',\n",
       " '2019.01.10.csv',\n",
       " '2019.01.11.csv',\n",
       " '2019.01.14.csv',\n",
       " '2019.01.15.csv',\n",
       " '2019.01.16.csv',\n",
       " '2019.01.17.csv',\n",
       " '2019.01.18.csv',\n",
       " '2019.01.21.csv',\n",
       " '2019.01.22.csv',\n",
       " '2019.01.23.csv',\n",
       " '2019.01.24.csv',\n",
       " '2019.01.25.csv',\n",
       " '2019.01.28.csv',\n",
       " '2019.01.29.csv',\n",
       " '2019.01.30.csv',\n",
       " '2019.01.31.csv',\n",
       " '2019.02.01.csv',\n",
       " '2019.02.11.csv',\n",
       " '2019.02.12.csv',\n",
       " '2019.02.13.csv',\n",
       " '2019.02.14.csv',\n",
       " '2019.02.15.csv',\n",
       " '2019.02.18.csv',\n",
       " '2019.02.19.csv',\n",
       " '2019.02.20.csv',\n",
       " '2019.02.21.csv',\n",
       " '2019.02.22.csv',\n",
       " '2019.02.25.csv',\n",
       " '2019.02.26.csv',\n",
       " '2019.02.27.csv',\n",
       " '2019.02.28.csv',\n",
       " '2019.03.01.csv',\n",
       " '2019.03.04.csv',\n",
       " '2019.03.05.csv',\n",
       " '2019.03.06.csv',\n",
       " '2019.03.07.csv',\n",
       " '2019.03.08.csv',\n",
       " '2019.03.11.csv',\n",
       " '2019.03.12.csv',\n",
       " '2019.03.13.csv',\n",
       " '2019.03.14.csv',\n",
       " '2019.03.15.csv',\n",
       " '2019.03.18.csv',\n",
       " '2019.03.19.csv',\n",
       " '2019.03.20.csv',\n",
       " '2019.03.21.csv',\n",
       " '2019.03.22.csv',\n",
       " '2019.03.25.csv',\n",
       " '2019.03.26.csv',\n",
       " '2019.03.27.csv',\n",
       " '2019.03.28.csv',\n",
       " '2019.03.29.csv',\n",
       " '2019.04.01.csv',\n",
       " '2019.04.02.csv',\n",
       " '2019.04.03.csv',\n",
       " '2019.04.04.csv',\n",
       " '2019.04.08.csv',\n",
       " '2019.04.09.csv',\n",
       " '2019.04.10.csv',\n",
       " '2019.04.11.csv',\n",
       " '2019.04.12.csv',\n",
       " '2019.04.15.csv',\n",
       " '2019.04.16.csv',\n",
       " '2019.04.17.csv',\n",
       " '2019.04.18.csv',\n",
       " '2019.04.19.csv',\n",
       " '2019.04.22.csv',\n",
       " '2019.04.23.csv',\n",
       " '2019.04.24.csv',\n",
       " '2019.04.25.csv',\n",
       " '2019.04.26.csv',\n",
       " '2019.04.29.csv',\n",
       " '2019.04.30.csv',\n",
       " '2019.05.06.csv',\n",
       " '2019.05.07.csv',\n",
       " '2019.05.08.csv',\n",
       " '2019.05.09.csv',\n",
       " 'results']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorise():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(7)\n",
    "                else:\n",
    "                    temp.append(8)\n",
    "            if i<0:\n",
    "                if i>self.threshold[0][2]:\n",
    "                    temp.append(4)\n",
    "                elif i>self.threshold[0][1]:\n",
    "                    temp.append(3)\n",
    "                elif i>self.threshold[0][0]:\n",
    "                    temp.append(2)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "        return np.asarray(temp)\n",
    "    \n",
    "class categorise_simple():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        self.threshold.append(np.percentile(array,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                else:\n",
    "                    temp.append(4)\n",
    "        return np.asarray(temp)    \n",
    "    \n",
    "class categorise_10():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[10,20,30,40,50,60,70,80,90]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(11)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(12)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(13)\n",
    "                elif i<self.threshold[1][3]:\n",
    "                    temp.append(14)\n",
    "                elif i<self.threshold[1][4]:\n",
    "                    temp.append(15)\n",
    "                elif i<self.threshold[1][5]:\n",
    "                    temp.append(16)\n",
    "                elif i<self.threshold[1][6]:\n",
    "                    temp.append(17)\n",
    "                elif i<self.threshold[1][7]:\n",
    "                    temp.append(18)\n",
    "                elif i<self.threshold[1][8]:\n",
    "                    temp.append(19)                    \n",
    "                else:\n",
    "                    temp.append(20)\n",
    "            if i<0:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                elif i<self.threshold[0][3]:\n",
    "                    temp.append(4)\n",
    "                elif i<self.threshold[0][4]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[0][5]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[0][6]:\n",
    "                    temp.append(7)\n",
    "                elif i<self.threshold[0][7]:\n",
    "                    temp.append(8)\n",
    "                elif i<self.threshold[0][8]:\n",
    "                    temp.append(9)                    \n",
    "                else:\n",
    "                    temp.append(10)\n",
    "        return np.asarray(temp)    \n",
    "class cross():\n",
    "    def __init__(self):\n",
    "        self.time_last_cross=0\n",
    "        self.current_sign=True\n",
    "        self.last_time=datetime(1900, 1, 1, 8, 59)\n",
    "    def get_time(self,time,price):\n",
    "        if (time-self.last_time)>timedelta(minutes=1):\n",
    "            self.last_time=time\n",
    "            self.time_last_cross=time\n",
    "            return 0\n",
    "        self.last_time=time\n",
    "        if (price>0) and self.current_sign : #if price positive and current trend is also positive\n",
    "            return (time-self.time_last_cross).total_seconds()\n",
    "        elif (price<0) and (not self.current_sign): #if price negative and current trend is negative\n",
    "            return (time-self.time_last_cross).total_seconds()\n",
    "        else: #if price positive, trend negative or price negative, trend positive\n",
    "            self.time_last_cross=time\n",
    "            self.current_sign=(price>0)\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smart_price(dataset):\n",
    "    data=dataset[:]\n",
    "    \n",
    "    #to combat the limit up event, where price is set to 0. \n",
    "    rows=(data.loc[:,'BidPrice1']==0) #count rows of bid price equal 0\n",
    "    if (np.any(rows)): #if there is such a row\n",
    "        data.at[rows,'BidPrice1']=data.loc[rows,'AskPrice1'] #for that row, assign ask price to it\n",
    "    rows=(data.loc[:,'AskPrice1']==0) #do the same for ask price\n",
    "    if (np.any(rows)):\n",
    "        data.at[rows,'AskPrice1']=data.loc[rows,'BidPrice1'] \n",
    "        \n",
    "    data['smart_price']=data.loc[:,'BidPrice1']*data.loc[:,'AskVol1']+data.loc[:,'AskPrice1']*data.loc[:,'BidVol1']\n",
    "    data.at[:,'smart_price']=data.loc[:,'smart_price']/(data.loc[:,['BidVol1','AskVol1']].sum(axis=1))  \n",
    "    return data\n",
    "\n",
    "def calc_present_vol(dataset):\n",
    "    data=dataset[:]\n",
    "    data['current_vol']=data.loc[:,'Volume'].diff().fillna(0)/2\n",
    "    return data\n",
    "\n",
    "def calc_future_price(dataset,time_ahead=30,time_index=44, price_col=-2):\n",
    "    data=dataset[:]\n",
    "    future_price=[]\n",
    "    length=len(data)\n",
    "    for i in range(len(data)):\n",
    "        current_time=data[i,time_index]+timedelta(seconds=time_ahead)\n",
    "        #print(data[i,44])\n",
    "        j=0\n",
    "        #print(current_time)\n",
    "        while((i+j)<length and current_time>data[(i+j),time_index]):\n",
    "            j+=1\n",
    "        if (i+j)<length:\n",
    "            future_price.append(data[(i+j),price_col]) \n",
    "        else:\n",
    "            future_price.append(np.nan)\n",
    "    future_price=np.asarray(future_price)\n",
    "    future_price=np.expand_dims(future_price,axis=1)\n",
    "    return np.concatenate((data,future_price),axis=1)\n",
    "\n",
    "\n",
    "def calc_edge(dataset,future_col=-1,current_col=-3):\n",
    "    data=dataset.copy()\n",
    "    temp=data[:,future_col]-data[:,current_col]\n",
    "    temp=np.expand_dims(temp,axis=1)\n",
    "    return np.concatenate((data,temp),axis=1)\n",
    "\n",
    "def set_index(dataset,time_index=44):\n",
    "    data=dataset[:]\n",
    "    index=data[:,time_index]\n",
    "    new_index=[]\n",
    "    for j in range(len(index)):\n",
    "        i=str(index[j]*1000)\n",
    "        if len(i)==11:\n",
    "            i='0'+i\n",
    "        i=i[:-10]+':'+i[-10:]\n",
    "        i=i[:-8]+':'+i[-8:]\n",
    "        i=i[:-6]+':'+i[-6:]\n",
    "        new_index.append(datetime.strptime(i,\"%H:%M:%S:%f\"))\n",
    "    data[:,time_index]=new_index\n",
    "    return data\n",
    "\n",
    "def calc_sma_fast(dataset,duration=1,time_index=44,price_col=-4): #faster way to calculate SMA, 0.05 seconds for 5000 rows\n",
    "    data=dataset[:]\n",
    "    sma_values=[] \n",
    "    smart_sum=np.cumsum(data[:,price_col]) #smart price column is -4\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,time_index]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,time_index]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            sma=(smart_sum[i]-smart_sum[i-j])/(j)\n",
    "            sma_values.append(sma)\n",
    "        else:\n",
    "            sma=smart_sum[i]/(i+1)\n",
    "            sma_values.append(sma)\n",
    "\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=data[:,price_col]-sma_values\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1)  \n",
    "\n",
    "def calc_past_vol(dataset,duration=1,time_index=44,vol_col=-4): #\n",
    "    data=dataset[:].copy()\n",
    "    vol_values=[] \n",
    "    vol_sum=np.cumsum(data[:,vol_col])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,time_index]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,time_index]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            vol=(vol_sum[i]-vol_sum[i-j])\n",
    "            vol_values.append(vol)\n",
    "        else:\n",
    "            vol=vol_sum[i]\n",
    "            vol_values.append(vol)\n",
    "    vol_values=np.asarray(vol_values)\n",
    "    vol_values=np.expand_dims(vol_values,axis=1)\n",
    "    return np.concatenate((data,vol_values),axis=1) \n",
    "\n",
    "def last_cross(dataset,time_index=44,price_col=-3):\n",
    "    data=dataset[:]\n",
    "    last_cross=cross()\n",
    "    timings=[]\n",
    "    for i in range(len(data)):\n",
    "        timings.append(last_cross.get_time(data[i,time_index],data[i,price_col]))\n",
    "    timings=np.asarray(timings)\n",
    "    timings=np.expand_dims(timings,axis=1)\n",
    "    return np.concatenate((data,timings),axis=1)\n",
    "def process(dataset,sma_duration=1,vol_duration=1,time_index=44):\n",
    "    data=dataset[:]\n",
    "    data=calc_smart_price(data).values #new\n",
    "    data=set_index(data,time_index=time_index) #no change\n",
    "    data=calc_future_price(data,time_index=time_index,price_col=-1) #new\n",
    "    data=calc_edge(data,future_col=-1,current_col=-2) #new\n",
    "    data=calc_sma_fast(data,duration=sma_duration,time_index=time_index,price_col=-3) #new\n",
    "    data=last_cross(data,time_index=time_index,price_col=-1) #new\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.02.csv read\n",
      "2019.01.03.csv read\n",
      "2019.01.04.csv read\n",
      "2019.01.07.csv read\n",
      "2019.01.08.csv read\n",
      "2019.01.09.csv read\n",
      "2019.01.10.csv read\n",
      "2019.01.11.csv read\n",
      "2019.01.14.csv read\n",
      "2019.01.15.csv read\n",
      "2019.01.16.csv read\n",
      "2019.01.17.csv read\n",
      "2019.01.18.csv read\n",
      "2019.01.21.csv read\n",
      "2019.01.22.csv read\n",
      "2019.01.23.csv read\n",
      "2019.01.24.csv read\n",
      "2019.01.25.csv read\n",
      "2019.01.28.csv read\n",
      "2019.01.29.csv read\n",
      "2019.01.30.csv read\n",
      "2019.01.31.csv read\n",
      "2019.02.01.csv read\n",
      "2019.02.11.csv read\n",
      "2019.02.12.csv read\n",
      "2019.02.13.csv read\n",
      "2019.02.14.csv read\n",
      "2019.02.15.csv read\n",
      "2019.02.18.csv read\n",
      "2019.02.19.csv read\n",
      "2019.02.20.csv read\n",
      "2019.02.21.csv read\n",
      "2019.02.22.csv read\n",
      "2019.02.25.csv read\n",
      "2019.02.26.csv read\n",
      "2019.02.27.csv read\n",
      "2019.02.28.csv read\n",
      "2019.03.01.csv read\n",
      "2019.03.04.csv read\n",
      "2019.03.05.csv read\n",
      "2019.03.06.csv read\n",
      "2019.03.07.csv read\n",
      "2019.03.08.csv read\n",
      "2019.03.11.csv read\n",
      "2019.03.12.csv read\n",
      "2019.03.13.csv read\n",
      "2019.03.14.csv read\n",
      "2019.03.15.csv read\n",
      "2019.03.18.csv read\n",
      "2019.03.19.csv read\n",
      "2019.03.20.csv read\n",
      "2019.03.21.csv read\n",
      "2019.03.22.csv read\n",
      "2019.03.25.csv read\n",
      "2019.03.26.csv read\n",
      "2019.03.27.csv read\n",
      "2019.03.28.csv read\n",
      "2019.03.29.csv read\n",
      "2019.04.01.csv read\n",
      "2019.04.02.csv read\n",
      "2019.04.03.csv read\n",
      "2019.04.04.csv read\n",
      "2019.04.08.csv read\n",
      "2019.04.09.csv read\n",
      "2019.04.10.csv read\n",
      "2019.04.11.csv read\n",
      "2019.04.12.csv read\n",
      "2019.04.15.csv read\n",
      "2019.04.16.csv read\n",
      "2019.04.17.csv read\n",
      "2019.04.18.csv read\n",
      "2019.04.19.csv read\n",
      "2019.04.22.csv read\n",
      "2019.04.23.csv read\n",
      "2019.04.24.csv read\n",
      "2019.04.25.csv read\n",
      "2019.04.26.csv read\n",
      "2019.04.29.csv read\n",
      "2019.04.30.csv read\n",
      "2019.05.06.csv read\n",
      "2019.05.07.csv read\n",
      "2019.05.08.csv read\n",
      "2019.05.09.csv read\n"
     ]
    }
   ],
   "source": [
    "#processing raw data to get technicals\n",
    "df_list=[]\n",
    "name_list=[]\n",
    "path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/'\n",
    "for file in file_list: #read all files and add them to file_list\n",
    "    if file[-3:]=='csv': #check if file is a CSV\n",
    "        name_list.append(file)\n",
    "        df_list.append(process(pd.read_csv(path+file),sma_duration=5))\n",
    "        print(file,'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.29.csv\n",
      "2019.01.30.csv\n",
      "2019.01.31.csv\n",
      "2019.02.01.csv\n",
      "2019.02.11.csv\n",
      "2019.02.12.csv\n",
      "2019.02.13.csv\n",
      "2019.02.14.csv\n",
      "2019.02.15.csv\n",
      "2019.02.18.csv\n",
      "2019.02.19.csv\n",
      "2019.02.20.csv\n",
      "2019.02.21.csv\n",
      "2019.02.22.csv\n",
      "2019.02.25.csv\n",
      "2019.02.26.csv\n",
      "2019.02.27.csv\n",
      "2019.02.28.csv\n",
      "2019.03.01.csv\n",
      "2019.03.04.csv\n",
      "2019.03.05.csv\n",
      "2019.03.06.csv\n",
      "2019.03.07.csv\n",
      "2019.03.08.csv\n",
      "2019.03.11.csv\n",
      "2019.03.12.csv\n",
      "2019.03.13.csv\n",
      "2019.03.14.csv\n",
      "2019.03.15.csv\n",
      "2019.03.18.csv\n",
      "2019.03.19.csv\n",
      "2019.03.20.csv\n",
      "2019.03.21.csv\n",
      "2019.03.22.csv\n",
      "2019.03.25.csv\n",
      "2019.03.26.csv\n",
      "2019.03.27.csv\n",
      "2019.03.28.csv\n",
      "2019.03.29.csv\n",
      "2019.04.01.csv\n",
      "2019.04.02.csv\n",
      "2019.04.03.csv\n",
      "2019.04.04.csv\n",
      "2019.04.08.csv\n",
      "2019.04.09.csv\n",
      "2019.04.10.csv\n",
      "2019.04.11.csv\n",
      "2019.04.12.csv\n",
      "2019.04.15.csv\n",
      "2019.04.16.csv\n",
      "2019.04.17.csv\n",
      "2019.04.18.csv\n",
      "2019.04.19.csv\n",
      "2019.04.22.csv\n",
      "2019.04.23.csv\n",
      "2019.04.24.csv\n",
      "2019.04.25.csv\n",
      "2019.04.26.csv\n",
      "2019.04.29.csv\n",
      "2019.04.30.csv\n",
      "2019.05.06.csv\n",
      "2019.05.07.csv\n",
      "2019.05.08.csv\n",
      "2019.05.09.csv\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_10split_time_cross_5min_sma_weightedmean.csv\n"
     ]
    }
   ],
   "source": [
    "#calculating results\n",
    "df_path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/'\n",
    "\n",
    "#create a list to hold all the data\n",
    "data_list=[]\n",
    "#creating a list for each category\n",
    "for i in range(20):\n",
    "    data_list.append([])\n",
    "    #creating a list for each quartile\n",
    "    for _ in range(4):\n",
    "        data_list[i].append([])\n",
    "\n",
    "final_df=pd.DataFrame()    \n",
    "\n",
    "for i in range(len(df_list)): #for each 20 day rolling window\n",
    "    if i<19: #skip first 19 days\n",
    "        continue\n",
    "    print(name_list[i])\n",
    "\n",
    "    #get -19 day\n",
    "    sma=df_list[i-19][:,-2].copy() #column for SMA\n",
    "    time_cross=df_list[i-19][:,-1].copy()\n",
    "    \n",
    "    #get -18 to 0 day (19 days in total)\n",
    "    for k in range((i-18),i+1): #get 20 day moving averages\n",
    "        sma=np.concatenate((sma,df_list[k][:,-2].copy()))\n",
    "        time_cross=np.concatenate((time_cross,df_list[k][:,-1].copy()))\n",
    "        \n",
    "    cat_sma=categorise_10()\n",
    "    cat_sma.fit(sma) #calculate quartile thresholds for past 20 days\n",
    "    \n",
    "    #get categories for past 20 days\n",
    "    cat_sma_20=(cat_sma.return_quartile(sma))   \n",
    "\n",
    "    #get x,y for regression\n",
    "    x_today=df_list[i][:,-2].copy().astype(float) #column for SMA     \n",
    "    y_today=df_list[i][:,-3].copy().astype(float) #column for edge\n",
    "    time_cross_today=df_list[i][:,-1].copy() #column for time since last cross\n",
    "    \n",
    "    #removing all NA\n",
    "    isnum=(~np.isnan(x_today)) & (~np.isnan(y_today))\n",
    "    #get categories of today's sma  \n",
    "    cat_x_today=cat_sma.return_quartile(x_today)\n",
    "    \n",
    "    #for each category\n",
    "    for cat in range(1,21):\n",
    "        #check past 20 days quartiles and filter time_cross by category\n",
    "        time_cross_fit=time_cross[cat_sma_20==cat].copy()\n",
    "\n",
    "        #simple quartile categorisation\n",
    "        cat_time=categorise_simple()\n",
    "        cat_time.fit(time_cross_fit)\n",
    "    \n",
    "        #today's sma filter\n",
    "        sma_filter_today=(cat_x_today==cat)\n",
    "        #today's time categories\n",
    "        cat_time_today=cat_time.return_quartile(time_cross_today)\n",
    "\n",
    "        #for each quartile\n",
    "        for quartile in [1,2,3,4]:\n",
    "\n",
    "            filtered= (isnum & sma_filter_today) #filtering NA and sma quartile\n",
    "            filtered= filtered & (cat_time_today==quartile) #filtering volume\n",
    "            new_y=y_today[filtered].copy()\n",
    "\n",
    "            if (len(new_y)!=0):\n",
    "\n",
    "                data_list[(cat-1)][(quartile-1)].append(new_y)\n",
    "reg_result={}                 \n",
    "for cat in range(20):\n",
    "    for quartile in range(4):\n",
    "        all_data=data_list[cat][quartile][0]\n",
    "        for i in range(1,(len(data_list[cat][quartile]))):\n",
    "            all_data=np.concatenate((all_data,data_list[cat][quartile][i]))\n",
    "        reg_result['category']=cat+1\n",
    "        reg_result['quartile']=quartile+1\n",
    "        reg_result['mean']=np.mean(all_data)\n",
    "        reg_result['std']=np.std(all_data)\n",
    "        reg_result['num obs']=len(all_data)\n",
    "        final_df=final_df.append(reg_result,ignore_index=True)  \n",
    "             \n",
    "temp=df_path+'result_10split_time_cross_5min_sma_weightedmean.csv'\n",
    "final_df.to_csv(temp)\n",
    "print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "[[-0.19267159  0.12752573  0.02202854  0.31957321]\n",
      " [-0.25649618  0.12531803  0.02666349 -0.0112897 ]\n",
      " [-0.1846334   0.03117279 -0.12877952  0.09156817]\n",
      " [-0.24075646 -0.08636318 -0.05001419 -0.22235719]\n",
      " [-0.27248255 -0.26193325 -0.03783178 -0.14843497]\n",
      " [-0.30906995 -0.23135513 -0.06245333 -0.01490423]\n",
      " [-0.21057097 -0.13290848 -0.1945946   0.0344212 ]\n",
      " [-0.2268676  -0.22584797 -0.03771642 -0.01787789]\n",
      " [-0.22338218 -0.31376897 -0.01367657  0.12721989]\n",
      " [-0.16607222 -0.22488814 -0.09246023  0.03626679]\n",
      " [ 0.1364673   0.11498446  0.0645534  -0.37262921]\n",
      " [ 0.31453199  0.18066004 -0.19912584 -0.18409987]\n",
      " [ 0.32758098  0.20738517  0.00116818 -0.0871107 ]\n",
      " [ 0.37557096  0.13040826 -0.00643352 -0.10886664]\n",
      " [ 0.34922263  0.12666693  0.16250199 -0.12311086]\n",
      " [ 0.37120529  0.16834032  0.0403432  -0.10191035]\n",
      " [ 0.45741924  0.18467509  0.1779754  -0.09037451]\n",
      " [ 0.2875211   0.32216369  0.14377845 -0.1403605 ]\n",
      " [ 0.38955228  0.25750677 -0.01039145 -0.08760104]\n",
      " [ 0.58069136 -0.1557486   0.25781341  0.04004206]]\n",
      "std\n",
      "[[5.5557898  4.19164812 4.21685733 4.29817099]\n",
      " [4.02443804 3.11933959 2.87106246 3.66163086]\n",
      " [3.50984057 2.72968434 2.54864307 2.91079298]\n",
      " [3.41238359 2.61730895 2.3647446  2.75399451]\n",
      " [3.19937021 2.64996688 2.23960735 2.50000275]\n",
      " [2.72740018 2.57329967 2.3403401  2.34692881]\n",
      " [2.79194441 2.33314472 2.16779375 2.24448899]\n",
      " [2.7191701  2.43819917 2.13449886 2.22720922]\n",
      " [2.58360194 2.40520857 2.03864725 2.08684303]\n",
      " [2.58816572 2.31551891 2.07747632 2.21015184]\n",
      " [2.56613563 2.26236951 2.30405752 2.26985402]\n",
      " [2.83730468 2.33978998 2.09914353 2.06432695]\n",
      " [2.7066908  2.35613028 2.21428599 2.29425495]\n",
      " [2.85822033 2.46405476 2.40901453 2.19714968]\n",
      " [3.13836699 2.50335943 2.31688366 2.32562268]\n",
      " [3.22236619 2.56585015 2.39426674 2.475063  ]\n",
      " [3.50470821 2.60353229 2.57419734 2.68430135]\n",
      " [3.66031451 2.91372202 2.86050555 2.97414209]\n",
      " [3.94250912 3.13921168 2.88586459 3.20740055]\n",
      " [5.01028018 3.57185193 4.70037079 4.48307187]]\n",
      "num obs\n",
      "[[37006. 34807. 34332. 33806.]\n",
      " [35430. 37168. 36941. 37194.]\n",
      " [35802. 39407. 37889. 38652.]\n",
      " [37435. 39566. 41310. 40646.]\n",
      " [37168. 40488. 41534. 41900.]\n",
      " [37772. 41059. 41664. 42582.]\n",
      " [38191. 41182. 42279. 43834.]\n",
      " [38228. 40767. 43653. 43192.]\n",
      " [38609. 41094. 44698. 43592.]\n",
      " [38352. 41165. 43860. 44455.]\n",
      " [34635. 41506. 42252. 42917.]\n",
      " [37244. 39880. 41784. 40711.]\n",
      " [37348. 40208. 40542. 41031.]\n",
      " [36831. 40321. 40933. 38459.]\n",
      " [36339. 39184. 41718. 38313.]\n",
      " [35505. 39360. 39717. 37847.]\n",
      " [35397. 38286. 37783. 37133.]\n",
      " [33438. 35469. 37700. 34476.]\n",
      " [35491. 36379. 35886. 33141.]\n",
      " [36087. 36035. 33544. 31776.]]\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_10split_time_cross_5min_sma_weightedmean.csv')\n",
    "mean=np.reshape(data.loc[:,'mean'].values,(20,4))\n",
    "std=np.reshape(data.loc[:,'std'].values,(20,4))\n",
    "num=np.reshape(data.loc[:,'num obs'].values,(20,4))\n",
    "print('mean')\n",
    "print(mean)\n",
    "print('std')\n",
    "print(std)\n",
    "print('num obs')\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
