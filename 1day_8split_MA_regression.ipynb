{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "os.chdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list=os.listdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '2019.01.02.csv',\n",
       " '2019.01.03.csv',\n",
       " '2019.01.04.csv',\n",
       " '2019.01.07.csv',\n",
       " '2019.01.08.csv',\n",
       " '2019.01.09.csv',\n",
       " '2019.01.10.csv',\n",
       " '2019.01.11.csv',\n",
       " '2019.01.14.csv',\n",
       " '2019.01.15.csv',\n",
       " '2019.01.16.csv',\n",
       " '2019.01.17.csv',\n",
       " '2019.01.18.csv',\n",
       " '2019.01.21.csv',\n",
       " '2019.01.22.csv',\n",
       " '2019.01.23.csv',\n",
       " '2019.01.24.csv',\n",
       " '2019.01.25.csv',\n",
       " '2019.01.28.csv',\n",
       " '2019.01.29.csv',\n",
       " '2019.01.30.csv',\n",
       " '2019.01.31.csv',\n",
       " '2019.02.01.csv',\n",
       " '2019.02.11.csv',\n",
       " '2019.02.12.csv',\n",
       " '2019.02.13.csv',\n",
       " '2019.02.14.csv',\n",
       " '2019.02.15.csv',\n",
       " '2019.02.18.csv',\n",
       " '2019.02.19.csv',\n",
       " '2019.02.20.csv',\n",
       " '2019.02.21.csv',\n",
       " '2019.02.22.csv',\n",
       " '2019.02.25.csv',\n",
       " '2019.02.26.csv',\n",
       " '2019.02.27.csv',\n",
       " '2019.02.28.csv',\n",
       " '2019.03.01.csv',\n",
       " '2019.03.04.csv',\n",
       " '2019.03.05.csv',\n",
       " '2019.03.06.csv',\n",
       " '2019.03.07.csv',\n",
       " '2019.03.08.csv',\n",
       " '2019.03.11.csv',\n",
       " '2019.03.12.csv',\n",
       " '2019.03.13.csv',\n",
       " '2019.03.14.csv',\n",
       " '2019.03.15.csv',\n",
       " '2019.03.18.csv',\n",
       " '2019.03.19.csv',\n",
       " '2019.03.20.csv',\n",
       " '2019.03.21.csv',\n",
       " '2019.03.22.csv',\n",
       " '2019.03.25.csv',\n",
       " '2019.03.26.csv',\n",
       " '2019.03.27.csv',\n",
       " '2019.03.28.csv',\n",
       " '2019.03.29.csv',\n",
       " '2019.04.01.csv',\n",
       " '2019.04.02.csv',\n",
       " '2019.04.03.csv',\n",
       " '2019.04.04.csv',\n",
       " '2019.04.08.csv',\n",
       " '2019.04.09.csv',\n",
       " '2019.04.10.csv',\n",
       " '2019.04.11.csv',\n",
       " '2019.04.12.csv',\n",
       " '2019.04.15.csv',\n",
       " '2019.04.16.csv',\n",
       " '2019.04.17.csv',\n",
       " '2019.04.18.csv',\n",
       " '2019.04.19.csv',\n",
       " '2019.04.22.csv',\n",
       " '2019.04.23.csv',\n",
       " '2019.04.24.csv',\n",
       " '2019.04.25.csv',\n",
       " '2019.04.26.csv',\n",
       " '2019.04.29.csv',\n",
       " '2019.04.30.csv',\n",
       " '2019.05.06.csv',\n",
       " '2019.05.07.csv',\n",
       " '2019.05.08.csv',\n",
       " '2019.05.09.csv',\n",
       " 'results']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorise():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def fit(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(7)\n",
    "                else:\n",
    "                    temp.append(8)\n",
    "            if i<0:\n",
    "                if i>self.threshold[0][2]:\n",
    "                    temp.append(4)\n",
    "                elif i>self.threshold[0][1]:\n",
    "                    temp.append(3)\n",
    "                elif i>self.threshold[0][0]:\n",
    "                    temp.append(2)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "        return np.asarray(temp)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def calc_vwap(dataset,duration=1): #to be implement\\n    data=dataset[:]\\n    for i in data[:,44]:\\n        last_time=i-timedelta(minutes=duration)\\n        rolling=data[(data[:,44]>=last_time) & (data[:,44]<i)]\\n        high=rolling[:,51].max()\\n        low=rolling[:,51].min()\\n        avg=(rolling[-1,51]+high+low)/3\\ndef calc_rsi(dataset)        '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_smart_price(dataset):\n",
    "    data=dataset[:]\n",
    "    \n",
    "    #to combat the limit up event, where price is set to 0. \n",
    "    rows=(data.loc[:,'BidPrice1']==0) #count rows of bid price equal 0\n",
    "    if (np.any(rows)): #if there is such a row\n",
    "        data.at[rows,'BidPrice1']=data.loc[rows,'AskPrice1'] #for that row, assign ask price to it\n",
    "    rows=(data.loc[:,'AskPrice1']==0) #do the same for ask price\n",
    "    if (np.any(rows)):\n",
    "        data.at[rows,'AskPrice1']=data.loc[rows,'BidPrice1'] \n",
    "        \n",
    "    data['smart_price']=data.loc[:,'BidPrice1']*data.loc[:,'AskVol1']+data.loc[:,'AskPrice1']*data.loc[:,'BidVol1']\n",
    "    data.at[:,'smart_price']=data.loc[:,'smart_price']/(data.loc[:,['BidVol1','AskVol1']].sum(axis=1))  \n",
    "    return data\n",
    "\n",
    "def calc_future_price(dataset,time_ahead=30):\n",
    "    data=dataset[:]\n",
    "    future_price=[]\n",
    "    length=len(data)\n",
    "    for i in range(len(data)):\n",
    "        current_time=data[i,44]+timedelta(seconds=time_ahead)\n",
    "        #print(data[i,44])\n",
    "        j=0\n",
    "        #print(current_time)\n",
    "        while((i+j)<length and current_time>data[(i+j),44]):\n",
    "            j+=1\n",
    "        #print(i,j,(data[(i+j-1),44]))\n",
    "        if (i+j)<length:\n",
    "            future_price.append(data[(i+j),51]) #51 is the index for smart price            \n",
    "        else:\n",
    "            future_price.append(np.nan)\n",
    "    future_price=np.asarray(future_price)\n",
    "    future_price=np.expand_dims(future_price,axis=1)\n",
    "    return np.concatenate((data,future_price),axis=1)\n",
    "\n",
    "\n",
    "def calc_edge(dataset):\n",
    "    data=dataset.copy()\n",
    "    temp=data[:,52]-data[:,51]\n",
    "    temp=np.expand_dims(temp,axis=1)\n",
    "    return np.concatenate((data,temp),axis=1)\n",
    "\n",
    "def set_index(dataset):\n",
    "    data=dataset[:]\n",
    "    index=data[:,44]\n",
    "    new_index=[]\n",
    "    for j in range(len(index)):\n",
    "        i=str(index[j]*1000)\n",
    "        if len(i)==11:\n",
    "            i='0'+i\n",
    "        i=i[:-10]+':'+i[-10:]\n",
    "        i=i[:-8]+':'+i[-8:]\n",
    "        i=i[:-6]+':'+i[-6:]\n",
    "        new_index.append(datetime.strptime(i,\"%H:%M:%S:%f\"))\n",
    "    data[:,44]=new_index\n",
    "    return data\n",
    "def calc_sma(dataset,duration=1): #not in use, just for reference\n",
    "    #5 seconds for 5000 rows\n",
    "    data=dataset[:]\n",
    "    sma_values=[]\n",
    "\n",
    "    for i in data[:,44]:\n",
    "        last_time=i-timedelta(minutes=duration)\n",
    "        sma=data[(data[:,44]>=last_time) & (data[:,44]<i)]\n",
    "        if len(sma)!=0:\n",
    "            sma=sma[:,51].mean()\n",
    "        else:\n",
    "            sma=np.nan\n",
    "        sma_values.append(sma)\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1)\n",
    "def calc_sma_fast(dataset,duration=1): #faster way to calculate SMA, 0.05 seconds for 5000 rows\n",
    "    data=dataset[:]\n",
    "    sma_values=[] \n",
    "    smart_sum=np.cumsum(data[:,51])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            sma=(smart_sum[i]-smart_sum[i-j])/(j)\n",
    "            sma_values.append(sma)\n",
    "        else:\n",
    "            sma=smart_sum[i]/(i+1)\n",
    "            sma_values.append(sma)\n",
    "\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=data[:,51]-sma_values\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1)        \n",
    "def process(dataset):\n",
    "    data=dataset[:]\n",
    "    data=calc_smart_price(data).values\n",
    "    data=set_index(data)\n",
    "    data=calc_future_price(data)\n",
    "    data=calc_edge(data)\n",
    "    data=calc_sma_fast(data,duration=1)\n",
    "    data=calc_sma_fast(data,duration=5)\n",
    "    data=calc_sma_fast(data,duration=15)\n",
    "    data=calc_sma_fast(data,duration=30)    \n",
    "    return data\n",
    "ma_dict={'-4':'1',\n",
    "        '-3':'5',\n",
    "        '-2':'15',\n",
    "        '-1':'30'}    \n",
    "#ignore below        \n",
    "'''def calc_vwap(dataset,duration=1): #to be implement\n",
    "    data=dataset[:]\n",
    "    for i in data[:,44]:\n",
    "        last_time=i-timedelta(minutes=duration)\n",
    "        rolling=data[(data[:,44]>=last_time) & (data[:,44]<i)]\n",
    "        high=rolling[:,51].max()\n",
    "        low=rolling[:,51].min()\n",
    "        avg=(rolling[-1,51]+high+low)/3\n",
    "def calc_rsi(dataset)        '''     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.02.csv read\n",
      "2019.01.03.csv read\n",
      "2019.01.04.csv read\n",
      "2019.01.07.csv read\n",
      "2019.01.08.csv read\n",
      "2019.01.09.csv read\n",
      "2019.01.10.csv read\n",
      "2019.01.11.csv read\n",
      "2019.01.14.csv read\n",
      "2019.01.15.csv read\n",
      "2019.01.16.csv read\n",
      "2019.01.17.csv read\n",
      "2019.01.18.csv read\n",
      "2019.01.21.csv read\n",
      "2019.01.22.csv read\n",
      "2019.01.23.csv read\n",
      "2019.01.24.csv read\n",
      "2019.01.25.csv read\n",
      "2019.01.28.csv read\n",
      "2019.01.29.csv read\n",
      "2019.01.30.csv read\n",
      "2019.01.31.csv read\n",
      "2019.02.01.csv read\n",
      "2019.02.11.csv read\n",
      "2019.02.12.csv read\n",
      "2019.02.13.csv read\n",
      "2019.02.14.csv read\n",
      "2019.02.15.csv read\n",
      "2019.02.18.csv read\n",
      "2019.02.19.csv read\n",
      "2019.02.20.csv read\n",
      "2019.02.21.csv read\n",
      "2019.02.22.csv read\n",
      "2019.02.25.csv read\n",
      "2019.02.26.csv read\n",
      "2019.02.27.csv read\n",
      "2019.02.28.csv read\n",
      "2019.03.01.csv read\n",
      "2019.03.04.csv read\n",
      "2019.03.05.csv read\n",
      "2019.03.06.csv read\n",
      "2019.03.07.csv read\n",
      "2019.03.08.csv read\n",
      "2019.03.11.csv read\n",
      "2019.03.12.csv read\n",
      "2019.03.13.csv read\n",
      "2019.03.14.csv read\n",
      "2019.03.15.csv read\n",
      "2019.03.18.csv read\n",
      "2019.03.19.csv read\n",
      "2019.03.20.csv read\n",
      "2019.03.21.csv read\n",
      "2019.03.22.csv read\n",
      "2019.03.25.csv read\n",
      "2019.03.26.csv read\n",
      "2019.03.27.csv read\n",
      "2019.03.28.csv read\n",
      "2019.03.29.csv read\n",
      "2019.04.01.csv read\n",
      "2019.04.02.csv read\n",
      "2019.04.03.csv read\n",
      "2019.04.04.csv read\n",
      "2019.04.08.csv read\n",
      "2019.04.09.csv read\n",
      "2019.04.10.csv read\n",
      "2019.04.11.csv read\n",
      "2019.04.12.csv read\n",
      "2019.04.15.csv read\n",
      "2019.04.16.csv read\n",
      "2019.04.17.csv read\n",
      "2019.04.18.csv read\n",
      "2019.04.19.csv read\n",
      "2019.04.22.csv read\n",
      "2019.04.23.csv read\n",
      "2019.04.24.csv read\n",
      "2019.04.25.csv read\n",
      "2019.04.26.csv read\n",
      "2019.04.29.csv read\n",
      "2019.04.30.csv read\n",
      "2019.05.06.csv read\n",
      "2019.05.07.csv read\n",
      "2019.05.08.csv read\n",
      "2019.05.09.csv read\n"
     ]
    }
   ],
   "source": [
    "file_list[1][-3:]\n",
    "df_list=[]\n",
    "name_list=[]\n",
    "path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/'\n",
    "for file in file_list: #read all files and add them to file_list\n",
    "    if file[-3:]=='csv': #check if file is a CSV\n",
    "        name_list.append(file)\n",
    "        df_list.append(process(pd.read_csv(path+file)))\n",
    "        print(file,'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ma\n",
      "starting 10: 20\n",
      "starting 10: 30\n",
      "starting 10: 40\n",
      "starting 10: 50\n",
      "starting 10: 60\n",
      "starting 10: 70\n",
      "starting 10: 80\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_1day_8split_30minute_ma.csv\n",
      "1 ma\n",
      "starting 10: 20\n",
      "starting 10: 30\n",
      "starting 10: 40\n",
      "starting 10: 50\n",
      "starting 10: 60\n",
      "starting 10: 70\n",
      "starting 10: 80\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_1day_8split_15minute_ma.csv\n",
      "2 ma\n",
      "starting 10: 20\n",
      "starting 10: 30\n",
      "starting 10: 40\n",
      "starting 10: 50\n",
      "starting 10: 60\n",
      "starting 10: 70\n",
      "starting 10: 80\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_1day_8split_5minute_ma.csv\n",
      "3 ma\n",
      "starting 10: 20\n",
      "starting 10: 30\n",
      "starting 10: 40\n",
      "starting 10: 50\n",
      "starting 10: 60\n",
      "starting 10: 70\n",
      "starting 10: 80\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_1day_8split_1minute_ma.csv\n"
     ]
    }
   ],
   "source": [
    "#run regressions against all 4 moving averages\n",
    "df_path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/'\n",
    "\n",
    "num_days=4\n",
    "for j in range(num_days): #for each column of moving average\n",
    "    \n",
    "    final_df=pd.DataFrame()    \n",
    "    print(j,'ma')    \n",
    "    j=j+1\n",
    "    j*=-1\n",
    "\n",
    "    for i in range(len(df_list)): #for each 20 day rolling window\n",
    "        if i<20:\n",
    "            continue\n",
    "        if i%10==0:\n",
    "            print('starting 10:',i)\n",
    "        #get -19 day\n",
    "        x=df_list[i-19][:,j]\n",
    "        #get -18 to 0 day (19 days in total)\n",
    "        for k in range((i-18),i+1): #get 20 day moving averages\n",
    "            x=np.concatenate((x,df_list[k][:,j]))\n",
    "\n",
    "        cat=categorise()\n",
    "        cat.fit(x) #calculate quartile thresholds for past 20 days\n",
    "        \n",
    "        x=df_list[i][:,j] #get x,y for regression\n",
    "        x=x.astype(float)        \n",
    "        y=df_list[i][:,-5]\n",
    "        y=y.astype(float)\n",
    "        \n",
    "        isnum=(~np.isnan(x)) & (~np.isnan(y))\n",
    "        y=y[isnum]#removing all NA\n",
    "        x=x[isnum]        \n",
    "        \n",
    "        category=cat.return_quartile(x)\n",
    "        \n",
    "        reg_result={}\n",
    "        reg_result['ma_time_minutes']=ma_dict.get(str(j))\n",
    "        reg_result['date']=name_list[i]\n",
    "        reg_result['total_obs']=len(x)\n",
    "        for quartile in range(1,9):\n",
    "            #filter by quartile\n",
    "            filtered=(category==quartile) \n",
    "            q='quartile_'+str(quartile)+'_'\n",
    "            if (np.any(filtered)):\n",
    "                new_x=x[filtered]\n",
    "                new_y=y[filtered] \n",
    "                result=stats.linregress(new_x,new_y)\n",
    "                reg_result[(q+'slope')]=result[0]\n",
    "                reg_result[(q+'intercept')]=result[1]\n",
    "                reg_result[(q+'r_val')]=result[2]\n",
    "                reg_result[(q+'p_val')]=result[3]\n",
    "                reg_result[(q+'std_err')]=result[4]\n",
    "                reg_result[(q+'x_mean')]=np.mean(new_x)\n",
    "                std=np.std(new_x)\n",
    "                reg_result[(q+'x_std')]=np.std(new_x)\n",
    "                reg_result[(q+'num_obs')]=len(new_x)\n",
    "\n",
    "            else:\n",
    "                reg_result[(q+'slope')]='na'\n",
    "                reg_result[(q+'intercept')]='na'\n",
    "                reg_result[(q+'r_val')]='na'\n",
    "                reg_result[(q+'p_val')]='na'\n",
    "                reg_result[(q+'std_err')]='na'\n",
    "                reg_result[(q+'x_mean')]='na'\n",
    "                reg_result[(q+'x_std')]='na'\n",
    "                reg_result[(q+'num_obs')]='0'\n",
    "        final_df=final_df.append(reg_result,ignore_index=True)                \n",
    "    temp=df_path+'result_1day_8split_'+ma_dict.get(str(j))+'minute_ma.csv'\n",
    "    final_df.to_csv(temp)\n",
    "    print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: 1 percentage positive: 0.3650794 mean slope: -0.0944691 std dev 0.2561066\n",
      "category: 2 percentage positive: 0.4603175 mean slope: 0.0002484 std dev 0.577241\n",
      "category: 3 percentage positive: 0.5873016 mean slope: 0.0562178 std dev 0.7733827\n",
      "category: 4 percentage positive: 0.6825397 mean slope: 0.3807183 std dev 0.8314094\n",
      "category: 5 percentage positive: 0.6507937 mean slope: 0.1983677 std dev 0.8524453\n",
      "category: 6 percentage positive: 0.5714286 mean slope: 0.0059226 std dev 0.6174626\n",
      "category: 7 percentage positive: 0.5238095 mean slope: -0.0049956 std dev 0.472878\n",
      "category: 8 percentage positive: 0.3968254 mean slope: -0.0651061 std dev 0.2246008\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_1day_8split_1minute_ma.csv')\n",
    "for i in range(1,9):\n",
    "    col='quartile_'+str(i)+'_slope'\n",
    "    row=data.loc[:,col]\n",
    "    pos=row[row>=0]\n",
    "    neg=row[row<0]\n",
    "    perc=len(pos)/(len(pos)+len(neg))\n",
    "    print('category:',i,'percentage positive:',round(perc,7),'mean slope:',round(row.mean(),7),'std dev',round(row.std(),7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
