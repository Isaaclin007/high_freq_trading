{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "os.chdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list=os.listdir('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA')\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '2019.01.02.csv',\n",
       " '2019.01.03.csv',\n",
       " '2019.01.04.csv',\n",
       " '2019.01.07.csv',\n",
       " '2019.01.08.csv',\n",
       " '2019.01.09.csv',\n",
       " '2019.01.10.csv',\n",
       " '2019.01.11.csv',\n",
       " '2019.01.14.csv',\n",
       " '2019.01.15.csv',\n",
       " '2019.01.16.csv',\n",
       " '2019.01.17.csv',\n",
       " '2019.01.18.csv',\n",
       " '2019.01.21.csv',\n",
       " '2019.01.22.csv',\n",
       " '2019.01.23.csv',\n",
       " '2019.01.24.csv',\n",
       " '2019.01.25.csv',\n",
       " '2019.01.28.csv',\n",
       " '2019.01.29.csv',\n",
       " '2019.01.30.csv',\n",
       " '2019.01.31.csv',\n",
       " '2019.02.01.csv',\n",
       " '2019.02.11.csv',\n",
       " '2019.02.12.csv',\n",
       " '2019.02.13.csv',\n",
       " '2019.02.14.csv',\n",
       " '2019.02.15.csv',\n",
       " '2019.02.18.csv',\n",
       " '2019.02.19.csv',\n",
       " '2019.02.20.csv',\n",
       " '2019.02.21.csv',\n",
       " '2019.02.22.csv',\n",
       " '2019.02.25.csv',\n",
       " '2019.02.26.csv',\n",
       " '2019.02.27.csv',\n",
       " '2019.02.28.csv',\n",
       " '2019.03.01.csv',\n",
       " '2019.03.04.csv',\n",
       " '2019.03.05.csv',\n",
       " '2019.03.06.csv',\n",
       " '2019.03.07.csv',\n",
       " '2019.03.08.csv',\n",
       " '2019.03.11.csv',\n",
       " '2019.03.12.csv',\n",
       " '2019.03.13.csv',\n",
       " '2019.03.14.csv',\n",
       " '2019.03.15.csv',\n",
       " '2019.03.18.csv',\n",
       " '2019.03.19.csv',\n",
       " '2019.03.20.csv',\n",
       " '2019.03.21.csv',\n",
       " '2019.03.22.csv',\n",
       " '2019.03.25.csv',\n",
       " '2019.03.26.csv',\n",
       " '2019.03.27.csv',\n",
       " '2019.03.28.csv',\n",
       " '2019.03.29.csv',\n",
       " '2019.04.01.csv',\n",
       " '2019.04.02.csv',\n",
       " '2019.04.03.csv',\n",
       " '2019.04.04.csv',\n",
       " '2019.04.08.csv',\n",
       " '2019.04.09.csv',\n",
       " '2019.04.10.csv',\n",
       " '2019.04.11.csv',\n",
       " '2019.04.12.csv',\n",
       " '2019.04.15.csv',\n",
       " '2019.04.16.csv',\n",
       " '2019.04.17.csv',\n",
       " '2019.04.18.csv',\n",
       " '2019.04.19.csv',\n",
       " '2019.04.22.csv',\n",
       " '2019.04.23.csv',\n",
       " '2019.04.24.csv',\n",
       " '2019.04.25.csv',\n",
       " '2019.04.26.csv',\n",
       " '2019.04.29.csv',\n",
       " '2019.04.30.csv',\n",
       " '2019.05.06.csv',\n",
       " '2019.05.07.csv',\n",
       " '2019.05.08.csv',\n",
       " '2019.05.09.csv',\n",
       " 'results']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorise():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def calc_thresholds(self,array): #get the percentile values of the array\n",
    "        self.threshold.append(np.percentile(array,self.percentiles))   \n",
    "     \n",
    "    def return_quartile(self,array): \n",
    "        temp=[]\n",
    "        for i in array:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                else:\n",
    "                    temp.append(4)             \n",
    "        return np.asarray(temp)\n",
    "    \n",
    "class categorise_sign():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[25,50,75]\n",
    "        \n",
    "    def calc_thresholds(self,array):\n",
    "        positive=array[array>0]\n",
    "        negative=array[array<0]\n",
    "        \n",
    "        self.threshold.append(np.percentile(negative,self.percentiles))   \n",
    "        self.threshold.append(np.percentile(positive,self.percentiles))\n",
    "     \n",
    "    def return_quartile(self,array):\n",
    "        temp=[]\n",
    "        for i in array:\n",
    "            if i>=0:\n",
    "                if i<self.threshold[1][0]:\n",
    "                    temp.append(5)\n",
    "                elif i<self.threshold[1][1]:\n",
    "                    temp.append(6)\n",
    "                elif i<self.threshold[1][2]:\n",
    "                    temp.append(7)\n",
    "                else:\n",
    "                    temp.append(8)\n",
    "            if i<0:\n",
    "                if i>self.threshold[0][2]:\n",
    "                    temp.append(4)\n",
    "                elif i>self.threshold[0][1]:\n",
    "                    temp.append(3)\n",
    "                elif i>self.threshold[0][0]:\n",
    "                    temp.append(2)\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "        return np.asarray(temp)\n",
    "                \n",
    "    \n",
    "class categorise_vol():\n",
    "    def __init__(self):\n",
    "        self.threshold=[]\n",
    "        self.percentiles=[10,25,50,75,90]\n",
    "        \n",
    "    def calc_thresholds(self,array): #get the percentile values of the array\n",
    "        self.threshold.append(np.percentile(array,self.percentiles))   \n",
    "     \n",
    "    def return_quartile(self,array): \n",
    "        temp=[]\n",
    "        for i in array:\n",
    "                if i<self.threshold[0][0]:\n",
    "                    temp.append(1)\n",
    "                elif i<self.threshold[0][1]:\n",
    "                    temp.append(2)\n",
    "                elif i<self.threshold[0][2]:\n",
    "                    temp.append(3)\n",
    "                elif i<self.threshold[0][3]:\n",
    "                    temp.append(4)\n",
    "                elif i<self.threshold[0][4]:\n",
    "                    temp.append(5)\n",
    "                else:\n",
    "                    temp.append(6)                    \n",
    "        return np.asarray(temp) \n",
    "    \n",
    "class LinearRegression(linear_model.LinearRegression):\n",
    "    \"\"\"\n",
    "    LinearRegression class after sklearn's, but calculate t-statistics\n",
    "    and p-values for model coefficients (betas).\n",
    "    Additional attributes available after .fit()\n",
    "    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n",
    "    which is (n_features, n_coefs)\n",
    "    This class sets the intercept to 0 by default, since usually we include it\n",
    "    in X.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
    "                     n_jobs=1):\n",
    "            self.fit_intercept = fit_intercept\n",
    "            self.normalize = normalize\n",
    "            self.copy_X = copy_X\n",
    "            self.n_jobs = n_jobs\n",
    "    def fit(self, X, y, n_jobs=1):\n",
    "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
    "\n",
    "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
    "        se = np.array([\n",
    "            np.sqrt(np.diagonal(sse[i] * np.linalg.inv(np.dot(X.T, X))))\n",
    "                                                    for i in range(sse.shape[0])\n",
    "                    ])\n",
    "\n",
    "        self.t = self.coef_ / se\n",
    "        self.p = 2 * (1 - stats.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1]))\n",
    "        return self    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def calc_vwap(dataset,duration=1): #to be implement\\n    data=dataset[:]\\n    for i in data[:,44]:\\n        last_time=i-timedelta(minutes=duration)\\n        rolling=data[(data[:,44]>=last_time) & (data[:,44]<i)]\\n        high=rolling[:,51].max()\\n        low=rolling[:,51].min()\\n        avg=(rolling[-1,51]+high+low)/3\\ndef calc_rsi(dataset)'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_smart_price(dataset):\n",
    "    data=dataset[:]\n",
    "    \n",
    "    #to combat the limit up event, where price is set to 0. \n",
    "    rows=(data.loc[:,'BidPrice1']==0) #count rows of bid price equal 0\n",
    "    if (np.any(rows)): #if there is such a row\n",
    "        data.at[rows,'BidPrice1']=data.loc[rows,'AskPrice1'] #for that row, assign ask price to it\n",
    "    rows=(data.loc[:,'AskPrice1']==0) #do the same for ask price\n",
    "    if (np.any(rows)):\n",
    "        data.at[rows,'AskPrice1']=data.loc[rows,'BidPrice1'] \n",
    "        \n",
    "    data['smart_price']=data.loc[:,'BidPrice1']*data.loc[:,'AskVol1']+data.loc[:,'AskPrice1']*data.loc[:,'BidVol1']\n",
    "    data.at[:,'smart_price']=data.loc[:,'smart_price']/(data.loc[:,['BidVol1','AskVol1']].sum(axis=1))  \n",
    "    return data\n",
    "\n",
    "def calc_present_vol(dataset):\n",
    "    data=dataset[:]\n",
    "    data['current_vol']=data.loc[:,'Volume'].diff().fillna(0)/2\n",
    "    return data\n",
    "\n",
    "def calc_future_price(dataset,time_ahead=30):\n",
    "    data=dataset[:]\n",
    "    future_price=[]\n",
    "    length=len(data)\n",
    "    for i in range(len(data)):\n",
    "        current_time=data[i,44]+timedelta(seconds=time_ahead)\n",
    "        #print(data[i,44])\n",
    "        j=0\n",
    "        #print(current_time)\n",
    "        while((i+j)<length and current_time>data[(i+j),44]):\n",
    "            j+=1\n",
    "        #print(i,j,(data[(i+j-1),44]))\n",
    "        if (i+j)<length:\n",
    "            future_price.append(data[(i+j),51]) #51 is the index for smart price            \n",
    "        else:\n",
    "            future_price.append(np.nan)\n",
    "    future_price=np.asarray(future_price)\n",
    "    future_price=np.expand_dims(future_price,axis=1)\n",
    "    return np.concatenate((data,future_price),axis=1)\n",
    "\n",
    "\n",
    "def calc_edge(dataset):\n",
    "    data=dataset.copy()\n",
    "    temp=data[:,53]-data[:,51]\n",
    "    temp=np.expand_dims(temp,axis=1)\n",
    "    return np.concatenate((data,temp),axis=1)\n",
    "\n",
    "def set_index(dataset):\n",
    "    data=dataset[:]\n",
    "    index=data[:,44]\n",
    "    new_index=[]\n",
    "    for j in range(len(index)):\n",
    "        i=str(index[j]*1000)\n",
    "        if len(i)==11:\n",
    "            i='0'+i\n",
    "        i=i[:-10]+':'+i[-10:]\n",
    "        i=i[:-8]+':'+i[-8:]\n",
    "        i=i[:-6]+':'+i[-6:]\n",
    "        new_index.append(datetime.strptime(i,\"%H:%M:%S:%f\"))\n",
    "    data[:,44]=new_index\n",
    "    return data\n",
    "\n",
    "def calc_sma_fast(dataset,duration=1): #0.05 seconds for 5000 rows\n",
    "    data=dataset[:]\n",
    "    sma_values=[] \n",
    "    smart_sum=np.cumsum(data[:,51])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            sma=(smart_sum[i]-smart_sum[i-j])/(j)\n",
    "            sma_values.append(sma)\n",
    "        else:\n",
    "            sma=smart_sum[i]/(i+1)\n",
    "            sma_values.append(sma)\n",
    "    sma_values=np.asarray(sma_values)\n",
    "    sma_values=data[:,51][:]-sma_values\n",
    "    sma_values=np.expand_dims(sma_values,axis=1)\n",
    "    return np.concatenate((data,sma_values),axis=1) \n",
    "\n",
    "def calc_past_vol(dataset,duration=1): #\n",
    "    data=dataset[:]\n",
    "    vol_values=[] \n",
    "    vol_sum=np.cumsum(data[:,52])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(minutes=duration)\n",
    "        j=220*duration#4x60=240\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            vol=(vol_sum[i]-vol_sum[i-j])\n",
    "            vol_values.append(vol)\n",
    "        else:\n",
    "            vol=vol_sum[i]\n",
    "            vol_values.append(vol)\n",
    "    vol_values=np.asarray(vol_values)\n",
    "    vol_values=np.expand_dims(vol_values,axis=1)\n",
    "    return np.concatenate((data,vol_values),axis=1) #52  \n",
    "\n",
    "def calc_past_vol_seconds(dataset,duration=10): #\n",
    "    data=dataset[:]\n",
    "    vol_values=[] \n",
    "    vol_sum=np.cumsum(data[:,52])\n",
    "    for i in range(len(data)):\n",
    "        last_time=data[i,44]-timedelta(seconds=duration)\n",
    "        j=2*duration#4 rows per second\n",
    "        while(i-j>0 and data[i-j,44]>last_time):\n",
    "            j+=1\n",
    "        if (i-j>=0):\n",
    "            vol=(vol_sum[i]-vol_sum[i-j])\n",
    "            vol_values.append(vol)\n",
    "        else:\n",
    "            vol=vol_sum[i]\n",
    "            vol_values.append(vol)\n",
    "    vol_values=np.asarray(vol_values)\n",
    "    vol_values=np.expand_dims(vol_values,axis=1)\n",
    "    return np.concatenate((data,vol_values),axis=1) #52  \n",
    "\n",
    "def process(dataset):\n",
    "    data=dataset[:]\n",
    "    data=calc_smart_price(data) #51\n",
    "    data=calc_present_vol(data).values #52\n",
    "    data=set_index(data)\n",
    "    data=calc_future_price(data) #53\n",
    "    data=calc_edge(data) #54\n",
    "    data=calc_sma_fast(data,duration=1) #55\n",
    "    data=calc_past_vol(data,duration=1) #56\n",
    "    return data\n",
    "\n",
    "ma_dict={'-4':'1',\n",
    "        '-3':'5',\n",
    "        '-2':'15',\n",
    "        '-1':'30'}    \n",
    "        \n",
    "'''def calc_vwap(dataset,duration=1): #to be implement\n",
    "    data=dataset[:]\n",
    "    for i in data[:,44]:\n",
    "        last_time=i-timedelta(minutes=duration)\n",
    "        rolling=data[(data[:,44]>=last_time) & (data[:,44]<i)]\n",
    "        high=rolling[:,51].max()\n",
    "        low=rolling[:,51].min()\n",
    "        avg=(rolling[-1,51]+high+low)/3\n",
    "def calc_rsi(dataset)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[]\n",
    "name_list=[]\n",
    "path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/'\n",
    "for file in file_list: #read all files and add them to file_list\n",
    "    if file[-3:]=='csv': #check if file is a CSV\n",
    "        name_list.append(file)\n",
    "        df_list.append(process(pd.read_csv(path+file)))\n",
    "        print(file,'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2019.01.29.csv\n",
      "20 2019.01.30.csv\n",
      "21 2019.01.31.csv\n",
      "22 2019.02.01.csv\n",
      "23 2019.02.11.csv\n",
      "24 2019.02.12.csv\n",
      "25 2019.02.13.csv\n",
      "26 2019.02.14.csv\n",
      "27 2019.02.15.csv\n",
      "28 2019.02.18.csv\n",
      "29 2019.02.19.csv\n",
      "30 2019.02.20.csv\n",
      "31 2019.02.21.csv\n",
      "32 2019.02.22.csv\n",
      "33 2019.02.25.csv\n",
      "34 2019.02.26.csv\n",
      "35 2019.02.27.csv\n",
      "36 2019.02.28.csv\n",
      "37 2019.03.01.csv\n",
      "38 2019.03.04.csv\n",
      "39 2019.03.05.csv\n",
      "40 2019.03.06.csv\n",
      "41 2019.03.07.csv\n",
      "42 2019.03.08.csv\n",
      "43 2019.03.11.csv\n",
      "44 2019.03.12.csv\n",
      "45 2019.03.13.csv\n",
      "46 2019.03.14.csv\n",
      "47 2019.03.15.csv\n",
      "48 2019.03.18.csv\n",
      "49 2019.03.19.csv\n",
      "50 2019.03.20.csv\n",
      "51 2019.03.21.csv\n",
      "52 2019.03.22.csv\n",
      "53 2019.03.25.csv\n",
      "54 2019.03.26.csv\n",
      "55 2019.03.27.csv\n",
      "56 2019.03.28.csv\n",
      "57 2019.03.29.csv\n",
      "58 2019.04.01.csv\n",
      "59 2019.04.02.csv\n",
      "60 2019.04.03.csv\n",
      "61 2019.04.04.csv\n",
      "62 2019.04.08.csv\n",
      "63 2019.04.09.csv\n",
      "64 2019.04.10.csv\n",
      "65 2019.04.11.csv\n",
      "66 2019.04.12.csv\n",
      "67 2019.04.15.csv\n",
      "68 2019.04.16.csv\n",
      "69 2019.04.17.csv\n",
      "70 2019.04.18.csv\n",
      "71 2019.04.19.csv\n",
      "72 2019.04.22.csv\n",
      "73 2019.04.23.csv\n",
      "74 2019.04.24.csv\n",
      "75 2019.04.25.csv\n",
      "76 2019.04.26.csv\n",
      "77 2019.04.29.csv\n",
      "78 2019.04.30.csv\n",
      "79 2019.05.06.csv\n",
      "80 2019.05.07.csv\n",
      "81 2019.05.08.csv\n",
      "82 2019.05.09.csv\n",
      "done /Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_Q1_to_8_1minSMA_6split_volume_nointercept.csv\n"
     ]
    }
   ],
   "source": [
    "#run regressions against all 4 moving averages\n",
    "df_path='/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/'\n",
    "\n",
    "final_df=pd.DataFrame()    \n",
    "\n",
    "for i in range(len(df_list)): #for each 20 day rolling window\n",
    "    if i<19:\n",
    "        continue #skip the first 19 rows as we want a 20 day rolling period\n",
    "    print(i,name_list[i])\n",
    "        \n",
    "    #get -19 day\n",
    "    sma_past=df_list[i-19][:,55][:].copy() #55 is sma\n",
    "    vol_past=df_list[i-19][:,56][:].copy() #56 is cumulative volume in past 1 minute\n",
    "    #get -18 to 0 day (19 days in total)\n",
    "    \n",
    "    for k in range((i-18),(i+1)): #get 20 day moving averages\n",
    "        sma_past=np.concatenate((sma_past,df_list[k][:,55].copy()))\n",
    "        vol_past=np.concatenate((vol_past,df_list[k][:,56].copy()))\n",
    "    #sma_past and vol_past are the past 20 days of sma and volume     \n",
    "\n",
    "    #calculate quartile thresholds for past 20 days, for sma and vol\n",
    "    cat_sma=categorise_sign()\n",
    "    cat_vol=categorise_vol()\n",
    "    cat_sma.calc_thresholds(sma_past) \n",
    "    cat_vol.calc_thresholds(vol_past)\n",
    "\n",
    "    #get x,y for regression\n",
    "    #55 is sma, 54 is edge, 56 is past 1 minute volume\n",
    "    x=df_list[i][:,55][:].astype(float).copy() \n",
    "    y=df_list[i][:,54][:].astype(float).copy() \n",
    "    vol=df_list[i][:,56][:].astype(float).copy()\n",
    "    \n",
    "    #removing all NA\n",
    "    isnum=(~np.isnan(x)) & (~np.isnan(y))\n",
    "    y=y[isnum]\n",
    "    x=x[isnum]        \n",
    "    vol=vol[isnum]\n",
    "    \n",
    "    #get the categories for sma and vol\n",
    "    category_sma=cat_sma.return_quartile(x)\n",
    "    category_vol=cat_vol.return_quartile(vol)\n",
    "    \n",
    "    reg_result={}\n",
    "    reg_result['date']=name_list[i]\n",
    "    \n",
    "    for sma_quartile in [1,2,3,4,5,6,7,8]:\n",
    "        \n",
    "        #filter by SMA first\n",
    "        name='sma_'+str(sma_quartile)+'_vol'\n",
    "        sma_filter=(category_sma==sma_quartile) #filter for SMA category\n",
    "        reg_result['num_obs_'+str(sma_quartile)]=len(x[sma_filter]) #num of obs in Q1 or Q8\n",
    "        \n",
    "        for vol_quartile in [1,2,3,4,5,6]:\n",
    "\n",
    "            q=name+str(vol_quartile)+'_'            \n",
    "            #filter by quartile\n",
    "            vol_filter=(category_vol==vol_quartile) #filter for volume category\n",
    "            filtered=(sma_filter&vol_filter) #combine both filters using &\n",
    "            \n",
    "            if (len(x[filtered])>1):\n",
    "                new_x=x[filtered] #using combined filters to get data for regression\n",
    "                new_y=y[filtered] \n",
    "                new_x=np.expand_dims(new_x,1)\n",
    "                new_y=np.expand_dims(new_y,1)\n",
    "                lin_model=LinearRegression(fit_intercept=False)\n",
    "                lin_model.fit(new_x,new_y)\n",
    "\n",
    "                reg_result[(q+'slope')]=lin_model.coef_[0][0]\n",
    "                reg_result[(q+'p_val')]=lin_model.p[0][0]\n",
    "                reg_result[(q+'x_mean')]=np.mean(new_x)\n",
    "                reg_result[(q+'x_std')]=np.std(new_x)\n",
    "                reg_result[(q+'num_obs')]=len(new_x)\n",
    "            else:\n",
    "                reg_result[(q+'slope')]='na'\n",
    "                reg_result[(q+'p_val')]='na'\n",
    "                reg_result[(q+'x_mean')]='na'\n",
    "                reg_result[(q+'x_std')]='na'\n",
    "                reg_result[(q+'num_obs')]='0'\n",
    "    final_df=final_df.append(reg_result,ignore_index=True)                \n",
    "temp=df_path+'result_Q1_to_8_1minSMA_6split_volume_nointercept.csv'\n",
    "final_df.to_csv(temp)\n",
    "print('done',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sma_1_vol1_slope percent positive 0.57143 mean 0.07873 std 0.58129\n",
      "sma_1_vol2_slope percent positive 0.5 mean 0.03297 std 0.28254\n",
      "sma_1_vol3_slope percent positive 0.51562 mean 0.02332 std 0.19355\n",
      "sma_1_vol4_slope percent positive 0.51562 mean 0.01266 std 0.16475\n",
      "sma_1_vol5_slope percent positive 0.53125 mean 0.02083 std 0.18885\n",
      "sma_1_vol6_slope percent positive 0.40323 mean -0.07593 std 0.22411\n",
      "sma_2_vol1_slope percent positive 0.73438 mean 0.31949 std 0.73591\n",
      "sma_2_vol2_slope percent positive 0.64062 mean 0.10861 std 0.34986\n",
      "sma_2_vol3_slope percent positive 0.625 mean 0.06296 std 0.20833\n",
      "sma_2_vol4_slope percent positive 0.65625 mean 0.11192 std 0.2476\n",
      "sma_2_vol5_slope percent positive 0.375 mean -0.08097 std 0.42557\n",
      "sma_2_vol6_slope percent positive 0.50794 mean 0.02843 std 0.6747\n",
      "sma_3_vol1_slope percent positive 0.73438 mean 0.309 std 0.76822\n",
      "sma_3_vol2_slope percent positive 0.64062 mean 0.2165 std 0.44379\n",
      "sma_3_vol3_slope percent positive 0.6875 mean 0.17881 std 0.41153\n",
      "sma_3_vol4_slope percent positive 0.5625 mean 0.13304 std 0.50925\n",
      "sma_3_vol5_slope percent positive 0.48438 mean -0.12961 std 0.88798\n",
      "sma_3_vol6_slope percent positive 0.48438 mean 0.00177 std 1.32908\n",
      "sma_4_vol1_slope percent positive 0.64062 mean 0.80583 std 1.96658\n",
      "sma_4_vol2_slope percent positive 0.78125 mean 0.71367 std 1.31197\n",
      "sma_4_vol3_slope percent positive 0.70312 mean 0.53437 std 1.07431\n",
      "sma_4_vol4_slope percent positive 0.625 mean 0.39042 std 1.13156\n",
      "sma_4_vol5_slope percent positive 0.45312 mean -0.44327 std 1.94448\n",
      "sma_4_vol6_slope percent positive 0.46875 mean -0.36684 std 4.10494\n",
      "sma_5_vol1_slope percent positive 0.45312 mean 0.32472 std 2.49168\n",
      "sma_5_vol2_slope percent positive 0.53125 mean 0.17844 std 1.20488\n",
      "sma_5_vol3_slope percent positive 0.53125 mean 0.06958 std 1.21044\n",
      "sma_5_vol4_slope percent positive 0.51562 mean 0.11215 std 1.29225\n",
      "sma_5_vol5_slope percent positive 0.54688 mean 0.64282 std 1.69089\n",
      "sma_5_vol6_slope percent positive 0.48387 mean -1.10435 std 5.34542\n",
      "sma_6_vol1_slope percent positive 0.57812 mean 0.04642 std 0.70788\n",
      "sma_6_vol2_slope percent positive 0.67188 mean 0.13972 std 0.46377\n",
      "sma_6_vol3_slope percent positive 0.6875 mean 0.13145 std 0.38344\n",
      "sma_6_vol4_slope percent positive 0.64062 mean 0.08124 std 0.50502\n",
      "sma_6_vol5_slope percent positive 0.60938 mean 0.09716 std 0.67671\n",
      "sma_6_vol6_slope percent positive 0.48387 mean 0.04885 std 1.67368\n",
      "sma_7_vol1_slope percent positive 0.5625 mean 0.23698 std 0.96455\n",
      "sma_7_vol2_slope percent positive 0.71875 mean 0.14692 std 0.33687\n",
      "sma_7_vol3_slope percent positive 0.59375 mean 0.09062 std 0.22163\n",
      "sma_7_vol4_slope percent positive 0.64062 mean 0.10463 std 0.24012\n",
      "sma_7_vol5_slope percent positive 0.54688 mean -0.00075 std 0.41757\n",
      "sma_7_vol6_slope percent positive 0.46032 mean 0.07303 std 0.71432\n",
      "sma_8_vol1_slope percent positive 0.4898 mean 0.10403 std 0.55969\n",
      "sma_8_vol2_slope percent positive 0.59375 mean 0.11715 std 0.44781\n",
      "sma_8_vol3_slope percent positive 0.65625 mean 0.09937 std 0.23453\n",
      "sma_8_vol4_slope percent positive 0.57812 mean 0.04118 std 0.19071\n",
      "sma_8_vol5_slope percent positive 0.48438 mean 0.00702 std 0.171\n",
      "sma_8_vol6_slope percent positive 0.46032 mean -0.05646 std 0.25949\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/hudsonyeo/Desktop/Python/leo/data/day/TA/results/result_Q1_to_8_1minSMA_6split_volume_nointercept.csv')\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "for i in [1,2,3,4,5,6,7,8]:\n",
    "    for j in range(1,7):\n",
    "        name='sma_'+str(i)+'_vol'+str(j)+'_slope'\n",
    "        series=data.loc[:,name]\n",
    "        x=[]\n",
    "        for k in data.loc[:,name]:\n",
    "            if is_number(k):\n",
    "                if k==k:\n",
    "                    x.append(float(k))\n",
    "        x=np.asarray(x)\n",
    "        pos=x[x>0]\n",
    "        print(name,'percent positive',round(len(pos)/len(x),5),'mean',round(np.mean(x),5),'std',round(np.std(x),5))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
